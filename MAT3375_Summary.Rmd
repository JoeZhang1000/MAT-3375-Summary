---
title: "MAT 3375 Summary"
author: "Joe Zhang"
date: "Fall 2023"
output: 
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

We want to model $Y$ in terms of $X$. We let $X_1, \dots, X_p$ be the explanatory variables and $Y$ be the response variable. We want to see how $Y$ changes with $X_1, \dots, X_p$. The relationship between the explanatory variables and the response variable can also be used for prediction the new value of $Y$ given new value of the explanatory variables. The primary goal in regression is to develop a model that relates the response to the explanatory variables, to test it, and ultimately to use it for inference and prediction.


# Simple Linear Regression

## The Model
We collect a set of paired data. We plot the $n$ paired data $Y_i$ vs. $X_i$. If it seems reasonable to fit a straight line to the points, we then postulate the following simple regression model

\begin{equation}
  Y_i = \beta_0 + \beta_1X_i + \epsilon_i
\end{equation}

In the model, $\epsilon$ represents an unobserved random error term, $\beta_0$ is the intercept, and $\beta_1$ is the slope of the line.  

Both $\beta_0$ and $\beta_1$ are labeled parameters. They need to be estimated usually from the observed data.  

Alternatively, the model may be expressed in terms of $(X_i - \overline{X})$

\begin{equation}
  Y_i = (\beta_0 + \beta_1\overline{X}) + \beta_1(X_i - \overline{X}) + \epsilon_i
\end{equation}

where $\overline{X}$ represents the average of the $X_i$.

The proposed model is linear in the parameters $\beta_0$ and $\beta_1$.  

The model would still be referred to as linear if instead we had $X_i^2$ instead of $X_i$.   
(i.e. The model $Y_i = \beta_0 + \beta_1X_i + \beta_2X_i^2 + \epsilon_i$ is still linear in the parameters).

## Model Assumptions

We assume the following: The random error terms are uncorrelated, have mean equal to 0, and common variance equal to $\sigma^2$. This assumption leads to the following:

\begin{itemize}
  \item $E[Y_i] = \beta_0 + \beta_1X_i$
  \item $Var[Y_i] = \sigma^2$
\end{itemize}  

Caution: A well fitting regression model does not imply causation.

## Least Squares Estimates

We define $Q$ as the sum of square errors

\begin{equation*}
\begin{split}
  Q 
  &= \sum_{i = 1}^{n} \epsilon_i^2\\
  &= \sum_{i = 1}^{n} [Y_i - \beta_0 - \beta_1X_i]^2
\end{split}
\end{equation*}

Then we need to find $\beta_0$ and $\beta_1$ such that they minimize $Q$. We do this by differentiating with respect to $\beta_0$ and $\beta_1$ and then setting the partial derivatives equal to 0. We get that the partial derivatives are:

\begin{equation*}
  \frac{\partial Q}{\partial \beta_0} = -2 \sum_{i = 1}^{n} [Y_i - \beta_0 - \beta_1X_i] = 0
\end{equation*}

\begin{equation*}
  \frac{\partial Q}{\partial \beta_1} = -2\sum_{i = 1}^{n}[Y_i - \beta_0 - \beta_1X_i]X_i = 0
\end{equation*}

By rearranging, we get the following equations:

\begin{equation*}
  \sum_{i = 1}^{n} [Y_i] = n\beta_0 + \beta_1 \sum_{i = 1}^{n}X_i
\end{equation*} 

\begin{equation*}
  \sum_{i = 1}^{n} [X_iY_i] = \beta_0 \sum_{i = 1}^{n} X_i + \beta_1\sum_{i = 1}^{n} X_i^2
\end{equation*}

Solving the system of linear equations, we let $b_0$ and $b_1$ represent the solutions to $\beta_0$ and $\beta_1$, respectively. We get

\begin{equation}
  b_0 = \overline{Y} - b_1\overline{X}
\end{equation}

\begin{equation}
  b_1 = \frac{\sum (X_i - \overline{X})Y_i}{\sum(X_i - \overline{X})^2}
\end{equation}

We can also express the equation of $b_1$ as

\begin{equation*}
  b_1 = \sum_{i = 1}^{n} k_iY_i
\end{equation*}

where $k_i = \frac{(X_i - \overline{X})}{\sum(X_i - \overline{X})^2}$

We have the following properties of the $k_i$:

\begin{itemize}
  \item $$\sum k_i = 0$$
  \item $$\sum k_iX_i = 1$$
  \item $$\sum k_i^2 = \frac{1}{\sum(X_i - \overline{X})^2}$$
\end{itemize}

To show the properties, we have that

\begin{equation*}
\begin{split}
  \sum k_i
  &= \frac{\sum (X_i - \overline{X})}{\sum (X_i - \overline{X})^2} \\
  &= \frac{(\sum X_i) - n\overline{X}}{\sum(X_i - \overline{X})^2} \\
  &= 0
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
  \sum k_iX_i
  &= \frac{\sum(X_i - \overline{X})X_i}{\sum(X_i - \overline{X})^2}\\
  &= \frac{\sum X_i^2 - \overline{X}\sum X_i}{\sum(X_i - \overline{X})^2} \\
  &= \frac{\sum X_i^2 - n\overline{X}}{\sum(X_i - \overline{X})^2} \\
  &= 1
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
  \sum k_i^2
  &= \frac{\sum(X_i - \overline{X})^2}{(\sum(X_i - \overline{X})^2)^2} \\
  &= \frac{1}{\sum (X_i - \overline{X})^2}
\end{split}
\end{equation*}

After finding the least squares estimate for $\beta_0$ and $\beta_1$, which we denote as $b_0$ and $b_1$, respectively, the line that fits the data is:

\begin{equation}
  \hat{Y} = b_0 + b_1X
\end{equation}

Alternatively, we can also have

\begin{equation*}
  \begin{split}
    \hat{Y}
    &= (b_0 + b_1\overline{X}) + b_1(X - \overline{X}) \\
    &= \overline{Y} - b_1\overline{X} + b_1\overline{X} + b_1(X - \overline{X}) \\
    &= \overline{Y} + b_1(X - \overline{X})
  \end{split}
\end{equation*}

It is also important to note that the point $(\overline{X}, \overline{Y})$ is on the line.

We can predict $Y$ using $X$ and the line.

## The Gauss-Markov Theorem

The Gauss-Markov Theorem states that the least squares estimators $b_0$ and $b_1$ are unbiased and have minimum variance among all unbiased linear estimators.

Recall: An estimator is unbiased if its expected value is the value of its parameter.

To show that $b_1$ is an unbiased estimator of $\beta_1$, we need to show that $E[b_1] = \beta_1$

\begin{equation*}
  \begin{split}
    E[b_1] 
    &= \sum k_iE[Y_i] \\
    &= \beta_0\sum k_i + \beta_1\sum k_iX_i \\
    &= \beta_0 \cdot 0 + \beta_1 \cdot 1 \\
    &= \beta_1
  \end{split}
\end{equation*}

To show that $b_0$ is an unbiased estimator of $\beta_0$, we need to show that $E[b_0] = \beta_0$

\begin{equation*}
  \begin{split}
    E[b_0]
    &= E[\overline{Y} - b_1\overline{X}] \\
    &= E[\overline{Y}] - E[b_1\overline{X}] \\
    &= \frac{1}{n}\sum E[Y_i] - \beta_1 \overline{X} \\
    &= \frac{1}{n} \sum (\beta_0 + \beta_1 X_i) - \beta_1 \overline{X} \\
    &= \beta_0 + \beta_1 \overline{X} - \beta_1 \overline{X} \\
    &= \beta_0
  \end{split}
\end{equation*}

Now, we want to show that $b_0$ and $b_1$ have minimum variance among all unbiased linear estimators.

Consider an unbiased estimator for $\beta_1$, say, $\hat{\beta_1} = \sum c_iY_i$, it must satisfy

\begin{equation*}
\begin{split}
  \beta_1 
  &= E[\hat{\beta_1}] \\
  &= \sum c_iE[Y_i] \\
  &= \sum c_i[\beta_0 + \beta_1X_i]
\end{split}
\end{equation*}

From this, we must have that $\sum c_i = 0$, $\sum c_iX_i = 1$, and $Var[\hat{\beta_1}] = \sigma^2\sum c_i^2$.

We set $c_i = k_i + d_i$ for arbitrary $d_i$. Then we get

\begin{equation*}
  \begin{split}
    \sum k_id_i
    &= \sum k_i(c_i - k_i) \\
    &= [\sum c_i \frac{(X_i - \overline{X})}{\sum(X_i - \overline{X})^2}] - \frac{1}{\sum(X_i - \overline{X})^2} \\
    &= [\frac{1}{\sum(X_i - \overline{X})^2} - 0] - \frac{1}{\sum(X_i - \overline{X})^2} \\
    &= 0
  \end{split}
\end{equation*}

If we define the vectors $\mathbf{c}^T = [c_1, c_2, ..., c_n]$, $\mathbf{k}^T = [k_1, k_2, ..., k_n]$, and $\mathbf{d}^T = [d_1, d_2, ..., d_n]$, we get that $\mathbf{k}^T \mathbf{d} = 0$. This shows that $\mathbf{k}$ and $\mathbf{d}$ have inner product 0 and are orthogonal vectors.

Since we have $c_i = k_i + d_i$, we get that $\mathbf{c} = \mathbf{k} + \mathbf{d}$. Since $\mathbf{k}$ and $\mathbf{d}$ are orthogonal, we have that by the Pythagorean theorem, $||\mathbf{c}||^2 = ||\mathbf{k}||^2  + ||\mathbf{d}||^2$. Then, we get that

\begin{equation*}
Var[\hat{\beta_1}] = \sigma^2 (\sum k_i^2 + \sum d_i^2)
\end{equation*}

The variance is minimized when $d_i$ are all 0. Then $\hat{\beta_1} = b_1$ since $c_i = k_i$.

## Summary of estimates

We may write $\hat{Y} = b_0 + b_1X$ for the estimated or fitted line, $e_i = Y_i - \hat{Y}_i$ for the estimated ith residual, and we estimate the variance $\sigma^2$ by

\begin{equation*}
  \hat{\sigma^2} = \frac{\sum e_i^2}{n-2}
\end{equation*}

This is also known as the mean square error or MSE. 

We have 

\begin{equation*}
  b_1 = \sum k_iY_i
\end{equation*}

\begin{equation*}
\begin{split}
  b_0 
  &= \overline{Y} - b_1\overline{X} \\
  &= \frac{\sum Y_i}{n} - \overline{X}\sum k_iY_i \\
  &= \sum (\frac{1}{n} - k_i\overline{X})Y_i
\end{split}
\end{equation*}

We also have the following propeties of the residuals:

\begin{itemize}
  \item $\sum e_i = 0$
  \item $\sum X_ie_i = 0$
\end{itemize}

To prove the properties, we have:

\begin{equation*}
\begin{split}
  \sum e_i
  &= \sum Y_i - \sum[\overline{Y} + b_1(X_i - \overline{X})]\\
  &= \sum (Y_i - \overline{Y}) \\
  &= 0
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
  \sum X_ie_i
  &= \sum X_iY_i - \overline{Y}\sum X_i - b_1 \sum X_i(X_i - \overline{X}) \\
  &= [\sum X_iY_i - n\overline{Y}\overline{X}] - \frac{\sum X_iY_i - n\overline{Y}\overline{X}}{\sum(X_i - \overline{X})^2} \sum X_i(X_i - \overline{X})\\
  &= 0
\end{split}
\end{equation*}

## The Geometry of Estimation

We let $\boldsymbol{X} = (X_1, ..., X_n)^T$, $\boldsymbol{Y} = (Y_1, ..., Y_n)^T$, $\boldsymbol{\hat{Y}} = (\hat{Y_1}, \hat{Y_2}, ..., \hat{Y_n})^T$

We let $\boldsymbol{e} = (e_1, e_2, ..., e_n)^T$ and $\boldsymbol{1_n} = (1, 1, ..., 1)$. Then we can find that $(\boldsymbol{X} - \overline{X}\boldsymbol{1_n})\boldsymbol{e} = 0$. From this, we know that the vector $\boldsymbol{e}$ is orthogonal to the vectors $\boldsymbol{1_n}$ and $\boldsymbol{X} - \overline{X}\boldsymbol{1_n}$. Since $\boldsymbol{\hat{Y}} = \overline{Y}\boldsymbol{1_n} + b_1(\boldsymbol{X} - \overline{X}\boldsymbol{1_n})$. From this, we get that $\boldsymbol{e}$ is orthogonal to $\boldsymbol{\hat{Y}}$.

Using this, we get the following result:

\begin{equation*}
  ||\boldsymbol{Y}||^2 = ||\boldsymbol{\hat{Y}}||^2 + ||\boldsymbol{e}||^2
\end{equation*}

Since we have that $\boldsymbol{\hat{Y}} = \overline{Y} \boldsymbol{1_n} + b_1(\boldsymbol{X} - \overline{X} \boldsymbol{1_n})$, we get that

\begin{equation*}
\begin{split}
  ||\boldsymbol{\hat{Y}}||^2 
  &= ||\overline{Y} \boldsymbol{1_n}||^2 + ||b_1 (\boldsymbol(X) - \overline{X} \boldsymbol{1_n})||^2\\
  &= \overline{Y}^2 \boldsymbol{1_n}^T \boldsymbol{1_n} + b_1^2 \sum (X_i - \overline{X})^2
\end{split}
\end{equation*}

Then we get that

\begin{equation*}
  \sum Y_i^2 = n \overline{Y}^2 + b_1^2 \sum (X_i - \overline{X})^2 + \sum (Y_i - \hat{Y_i})^2
\end{equation*}

From that, we get

\begin{equation}
\sum (Y_i - \overline{Y})^2 = b_1^2 \sum (X_i - \overline{X})^2 + \sum (Y_i - \hat{Y_i})^2
\end{equation}

We call $\sum (Y_i - \overline{Y})^2$ the total sum of squares, $b_1^2 \sum (X_i - \overline{X})^2$ the regression sum of squares, and $\sum (Y_i - \hat{Y_i})^2$ the error sum of squares. This can be used for inferences in regression, which we will talk about in the next section.

## Inference in regression

Remark: If we assume that the random errors ${\epsilon_i} \sim N(0, \sigma^2)$, then we get that the likelihood function is 

\begin{equation*}
  L(\beta_0, \beta_1, \sigma^2) = (\frac{1}{\sqrt{2\pi}\sigma})^n e^{\frac{1}{2\sigma^2} \sum \epsilon_i^2}
\end{equation*}

Maximizing this function is equivalent to minimizing $Q = \sum epsilon_i^2$, we get the same results for $\beta_0$ and $\beta_1$.

We can also obtain an estimate for $\sigma^2$. It can be estimated by $MSE = \frac{\sum e_i^2}{n-2}$.

Suppose we have the model $Y_i = \beta_0 + \beta_1X_i + \epsilon_i$, where $\epsilon_i \sim N(0, \sigma^2)$ for $i = 1, ..., n$. Then we have

\begin{itemize}
  \item $\frac{b_1 - \beta_1}{s(b_1)} \sim t_{n-2}$ where $s^2(b_1) = \frac{MSE}{\sum(X_i - \overline{X})^2}$
  \item $\frac{b_0 - \beta_0}{s(b_0)} \sim t_{n-2}$ where $s^2(b_0) = MSE(\frac{1}{n} + \frac{\overline{X}^2}{\sum(X_i - \overline{X})^2})$
  \item MSE is an unbiased estimate of $\sigma^2$ and $\frac{(n-2)MSE}{\sigma^2} \sim \chi^2_{n-2}$
\end{itemize}

We can use the properties above to construct confidence intervals for the parameters and test hypotheses. We get that

\begin{itemize}
  \item $100(1-\alpha)\% \text{ CI for }\beta_1: b_1 \pm t_{n-2}(\frac{\alpha}{2}) s(b_1)$
  \item $100(1-\alpha)\% \text{ CI for }\beta_0: b_0 \pm t_{n-2}(\frac{\alpha}{2}) s(b_0)$
\end{itemize}

We can also test hypotheses such as $H_0: \beta_1 = 0$ vs. $H_1: \beta_1 \neq 0$ using the test statistic $T = \frac{b_1}{s(b_1)} \sim t_{n-2}$.

## Example for regression

We consider the following example on grade point averages at the end of the freshman year (Y) as a function of the ACT test scores (X). 

\begin{itemize}
  \item We plot the data
  \item We obtain the least squares estimates
  \item We plot the estimated regression function and estimate Y when $X = 30$
\end{itemize}

The R code below will complete the actions

```{r}
data = read.table("/Users/joezhang/Downloads/Grade point average.txt", header = TRUE, sep = '\t')
names(data)
GPA = data$GPA
ACT = data$ACT
fit = lm(GPA~ACT, data = data)
fit
```
The number under (Intercept) is the least squares estimate for $\beta_0$ and the number under ACT is the least squares estimate for $\beta_1$.

The code below constructs a 95% confidence interval for both $\beta_0$ and $\beta_1$.

```{r}
confint(fit, level = 0.95)
```

The code below plots the data and also constructs a 95% confidence interval and 95% prediction interval for the average of Y.

```{r}
library(ggplot2)
ggplot(data, aes(x = ACT, y = GPA)) +
  geom_point()+
  geom_smooth(method = lm, se = TRUE)
temp_var = predict(fit, interval = 'prediction')
new_df = cbind(data, temp_var)
ggplot(new_df, aes(ACT, GPA))+
  geom_point()+
  geom_line(aes(y = lwr), color = 'red', linetype = 'dashed')+
  geom_line(aes(y = upr), color = 'red', linetype = 'dashed')+
  geom_smooth(method = lm, se = TRUE)
```

## Analysis of Variance (ANOVA)

Below is the typical format of an analysis of variance (ANOVA) table (for this part, we use $p = 2$):

\begin{table}[!h]
\centering
\caption{ANOVA Table}
\begin{tabular}{|c|c|c|c|c|c|}

\hline
Source & Sum of Squares (SS) & df & Mean Square (MS = SS/df) & F statistic & E[MS] \\
\hline
Regression & $SSR = b_1^2\sum(X_i - \overline{X})^2$ & $p - 1$ & $MSR = \frac{SSR}{p-1}$ & $\frac{MSR}{MSE}$ & $\sigma^2 + \beta_1^2 \sum(X_i - \overline{X})^2$ \\
\hline
Error & $SSE = \sum (Y_i - \hat{Y_i})^2$ & $n - p$ & $MSE = \frac{SSE}{n-p}$ & & $\sigma^2$\\
\hline
& & & & & \\
\hline
Total & $SSTO = \sum(Y_i - \overline{Y})^2$ & $n - 1$ & & &\\
\hline
  
\end{tabular}
\label{lyxtab2}
\end{table}


Each of the sums of squares is a quadratic form where the rank of the corresponding matrix is the degrees of freedom indicated. Cochran's theorem applies and we conclude that the quadratic forms are independent and have Chi-Square distributions. It is well known that the ratio of the two independent Chi-Square divided by their degrees of freedom has a F-distribution (To be seen in section 3 of the notes).

We get that 

\begin{itemize}
  \item $\frac{SSR}{\sigma^2} \sim \chi^2(p-1)$
  \item $\frac{SSE}{\sigma^2} \sim \chi^2(n-p)$
\end{itemize} 

Then, we get that the F statistic is

\begin{equation*}
  F = \frac{SSR/(\sigma^2 (p-1))}{SSE/(\sigma^2 (n-p))} = \frac{SSR/(p-1)}{SSE/(n-p)} = \frac{MSR}{MSE} \sim F(p-1, n-p)
\end{equation*}

The degrees of freedom are determined by how much data is required to calculate a particular expression.

$\sum (Y_i - \overline{Y})^2$ has $n-1$ degrees of freedom because of the constraints that $\sum (Y_i - \overline{Y}) = 0$

$b_1^2 \sum (X_i - \overline{X})^2$ has one degree of freedom because it is a function of $b_1$

$\sum (Y_i - \hat{Y_i})^2$ has $n-2$ degrees of freedom because it is a function of two parameters.

We'll prove all these using matrices in section 3.

## Testing with ANOVA table

We can use the ANOVA table to test the hypotheses $H_0: \beta_1 = 0$ versus $H_1: \beta_1 \neq 0$. The null hypothesis states that the slope of the line is equal to 0. Under the null hypothesis, the expected mean square for regression and the expected mean square error are separate independent estimates of the variance $\sigma^2$. Hence, if the null hypothesis is true, the F-ratio should be small. On the other hand, if the alternative hypothesis $H_1$ is true, then the numerator of the F ration will be expected to be large. Consequently, large values of the F statistic are consistent with the alternative. We reject the null hypothesis for large values of F.

In other words, under the null hypothesis, we have that $E[MSR] = \sigma^2$ and $E[MSE] = \sigma^2$. Then the F ratio $F = \frac{MSR}{MSE}$ would be close to 1. Under the alternative hypothesis, $E[MSE] = \sigma^2$. However, $E[MSR] = \sigma^2 + \beta_1^2\sum(X_i - \overline{X})^2$ since $\beta_1 \neq 0$. Therefore, the F ratio is expected to be large. This is why we reject $H_0$ for large values of the F ratio. 

## Back to GPA data

If we consider the GPA data, we can construct an ANOVA table. We do this using R.

```{r}
anova(fit)
```
This shows that the F value is large and the p-value is small. We can reject $H_0$ in this case. This means that there is convincing evidence that the slope is not 0 and there is a relationship between the ACT score and GPA.

Now, we want to construct a 95\% confidence interval for $\beta_0$ and $\beta_1$ for the GPA data using the data summary.

```{r}
summary(fit)
```

We get that a confidence interval for $\beta_0$ can be calculated the following way: 

CI for $\beta_0$: $b_0 \pm t_{\alpha/2}{117} \cdot s(b_0) = 2.14596 \pm 1.98(0.32318) = (1.5059, 2.7860)$

We get that a confidence interval for $\beta_1$ can be calculated the following way:

CI for $\beta_1$: $b_1 \pm t_{\alpha/2}{117} \cdot s(b_1) = 0.03735 \pm 1.98(0.01289) = (0.01181, 0.06288)$

We can do hypothesis testing using t statistics on both $\beta_0$ and $\beta_1$. 

If we test $H_0: \beta_0 = 0$ versus $H_1: \beta_0 \neq 0$, we can use the R output and we find that $t = 6.640$, which is significant. We can then reject $H_0$. Similar with $\beta_1$.

However, if we want to test $H_0: \beta_0 = \beta_{0_1}$ versus $H_1: \beta_0 \neq \beta_{0_1}$ for some $\beta_{0_1} \neq 0$, then we can't use R. We have to use the test statistic $t = \frac{b_0 - \beta_{0_1}}{s(b_0)} \sim t_{n-2}$ to test and this cannot be computed using R. Similar for $\beta_1$.

## Confidence Interval for mean of Y for a given X

We want to construct a confidence interval for the mean of $Y^*$ at a given $X^*$, or $E[Y^*]$.

To estimate $E[Y^*]$, we know that $E[Y^*] = \beta_0 + \beta_1X^*$. We can estimate $E[Y^*]$ by
$$\hat{Y}^* = b_0 + b_1X^* = \sum(\frac{1}{n} + k_i(X^* - \overline{X}))Y_i$$ for a given value of $X^*$. The estimator is unbiased and has a normal distribution.

We also get that 

\begin{equation*}
\begin{split}
  Var[\hat{Y}^*]
  &= \sigma^2 \sum(\frac{1}{n} + k_i(X^* - \overline{X}))^2\\
  &= \sigma^2 \sum((\frac{1}{n})^2 + k_i^2(X^* - \overline{X}) + 2(\frac{1}{n})k_i(X^* - \overline{X}))\\
  &= \sigma^2(\frac{1}{n} + \frac{(X^* - \overline{X})^2}{\sum (X_i - \overline{X})^2})
\end{split}
\end{equation*}

The variance of $\hat{Y}^*$ can be estimated by $s^2[\hat{Y}^*] = MSE(\frac{1}{n} + \frac{(X^* - \overline{X})^2}{\sum (X_i - \overline{X})^2})$

We can then use the fact that $\frac{\hat{Y}^* - E[Y^*]}{s[\hat{Y^*}]} \sim t_{n-2}$ to make inference on $E[Y]$. We can then construct a $100(1-\alpha)\%$ confidence interval for $E[Y^*]$ by $\hat{Y}^* \pm t_{\alpha/2, t-2}s[\hat{Y}^*]$.

The width of the confidence interval is different at different values of $X^*$. In fact, the interval is the narrowest at $X^* = \overline{X}$ and gets wider as it deviates from $\overline{X}$.

## Prediction Interval for Y for a given X

For prediction, we want to find a confidence interval for a new value of $Y^*$ for a given $X^*$.

Note: Alvo's explanations don't make sense. I used the textbook, internet resources, and Boily's notes to make this section. Please let me know if there's anything I need to correct.

We consider the random variable $Y^* - \hat{Y}^*$ for a given $X^*$. We can use this to make inferences on the predicted value of $Y^*$.

We have that $E[Y^* - \hat{Y}^*] = 0$. To show this, we have that

\begin{equation*}
\begin{split}
  E[Y^* - \hat{Y}^*]
  &= E[Y^*] - E[\hat{Y}^*] \\
  &= \beta_0 + \beta_1X^* - E[b_0 + b_1X^*] \\
  &= \beta_0 + \beta_1X^* - E[b_0] - E[b_1]X^* \\
  &= \beta_0 + \beta_1X^* - \beta_0 - \beta_1X^* \\
  &= 0
\end{split}
\end{equation*}

We also have that $Var[Y^* - \hat{Y}^*] = \sigma^2(1+\frac{1}{n} + \frac{X^* - \overline{X}}{\sum(X_i - \overline{X})^2})$. To show this, we have

\begin{equation*}
\begin{split}
  Var[Y^* - \hat{Y}^*]
  &= Var[Y^*] + Var[\hat{Y}^*] \\
  &= \sigma^2 + \sigma^2(\frac{1}{n} + \frac{(X^* - \overline{X})}{\sum (X_i - \overline{X})}) \\
  &= \sigma^2 (1 + \frac{1}{n} + \frac{(X^* - \overline{X})^2}{\sum (X_i - \overline{X})^2})
\end{split}
\end{equation*}

Then we have that $Y^* - \hat{Y}^* \sim N(0, \sigma^2 (1 + \frac{1}{n} + \frac{(X^* - \overline{X})}{\sum (X_i - \overline{X})}))$

We estimate the variance of $Y^* - \hat{Y}^*$ by 

\begin{equation*}
  s^2[Y^* - \hat{Y}^*] = MSE(1 + \frac{1}{n} + \frac{ (X^* - \overline{X})^2}{\sum (X_i - \overline{X})^2})
\end{equation*}

Then we get that 

\begin{equation*}
  \frac{(Y^* - \hat{Y}^*) - 0}{s[Y^* - \hat{Y}^*]} \sim t_{n-2}
\end{equation*}

Then we can construct a prediction interval for $Y^*$. The prediction interval is $\hat{Y}^* \pm t_{\alpha/2; n-2}s(Y^* - \hat{Y}^*)$

## Example: Airfreight Data

```{r, include=FALSE, echo=FALSE}
library(knitr)
```

```{r}
data = read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Airfreight Data.txt", header=TRUE, sep = '\t')
kable(data)
```

a. Compute the ANOVA table.
b. Compute confidence intervals for the parameters.
c. Compute a confidence interval for the average response when X = 1.

To compute an ANOVA, table, we simply use the r command

```{r}
x = data$Shipment.Route
y = data$Airfreight.breakage
fit = lm(y~x)
anova(fit)
```

We conclude that the regression is highly significant since the F statistic has a value of 72.73.

We now want to compute a confidence interval for the coefficients, we do this using the following R command:

```{r}
summary(fit)
```

We get that for $\beta_0$, a $100(1-\alpha)\%$ confidence interval is $10.2000 \pm t_{\alpha/2, 8} \cdot 0.6633$. For $\beta_1$, a $100(1-\alpha)\%$ confidence interval is $4.0000 \pm t_{\alpha/2, 8} \cdot 0.4690$. In addition, we get that $\hat{\sigma^2} = 2.2$ on 8 degrees of freedom. 

To compute a 95\% confidence interval for the average response when $X = 1$, we can use the following R commands:

```{r}
new.dat = data.frame(x=1)
predict(fit, newdata=new.dat, interval="confidence")
```

To compute a 95\% prediction interval for Y at $X = 1$, we can use the following R commands:

```{r}
new.dat = data.frame(x=1)
predict(fit, newdata=new.dat, interval='prediction')
```

## Correlation Coefficient

The sample correlation coefficient is defined the following way:

\begin{equation}
  r = \frac{\sum (X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum(X_i - \overline{X})^2 \sum(Y_i - \overline{Y})}}
\end{equation}

The correlation coefficient is related to $b_1$. We can rewrite the equation as

\begin{equation*}
  r = b_1(\frac{\sum(X_i - \overline{X})^2}{\sum(Y_i - \overline{Y})^2})^{\frac{1}{2}}
\end{equation*}

The population correlation coefficient is denoted by $\rho$. It is

\begin{equation*}
  \rho = \frac{Cov(X, Y)}{\sqrt{Var[X]Var[Y]}}
\end{equation*}

We use r to estimate $\rho$.

Under $H_0: \rho = 0$, we have that $$t = \frac{r \sqrt{n-2}}{\sqrt{1-r^2}} \sim t_{n-2}$$

We can perform a test for $\rho$ using the R command:

```{r, include=FALSE}
library(MPV)
```

```{r}
x = p2.10$sysbp
y = p2.10$weight
cor.test(x, y, NULL, method = "pearson")
```

If we test $H_0: \rho = \rho_0$, then we use the following fact to make inference:

\begin{equation*}
  Z = arctanh(r) = \frac{1}{2}\cdot ln(\frac{1+r}{1-r}) \sim N(arctanh(\rho), \frac{1}{n-3})
\end{equation*}

So if we want to test the hypothesis, we use the test statistic: $Z = (arctanh(r) - arctanh(\rho_0))\sqrt{n-3}$. 

We reject $H_0$ for large values of the test statistic.

To compute a confidence interval of $\rho$, we use the following formula: $[\tanh(arctanh(r) - z_{\alpha/2}(n-3)^{\frac{1}{2}}), tanh(arctanh(r) + z_{\alpha/2}(n-3)^{\frac{1}{2}})]$



# Matrix Approach to Regression

## Matrix Notations

If we let $\boldsymbol{Y} = [Y_1, ..., Y_n]^T$ be the transpose of the column data vector, then we define the expectation by $\boldsymbol{E[Y]} = [E[Y_1], ..., E[Y_n]]^T$.

Proposition: If $\boldsymbol{Z} = \boldsymbol{AY} + \boldsymbol{B}$ for some matrix of constants $\boldsymbol{A}$, $\boldsymbol{B}$, then we have $\boldsymbol{E[Z]} = \boldsymbol{AE[Y] + B}$. 

To prove this, we let $\boldsymbol{Z} = [Z_1, ..., Z_n]^T$, $a_{ij}$ be the element of the matrix $\boldsymbol{A}$ in the i-th row and j-th column. Let $\boldsymbol{B} = [b_1, ..., b_n]$. Then we get

\begin{equation*}
  \begin{split}
    E[Z_i]
    &= E\{[\sum_j a_{ij}Y_j + b_i]\} \\
    &= [\sum_j a_{ij}E[Y_j]] + b_i
  \end{split}
\end{equation*}

We define the covariance of $\boldsymbol{Y}$, or the variance-covariance matrix of $Y$, denoted by $Cov[\boldsymbol{Y}]$, by $$Cov[\boldsymbol{Y}] = E\{[\boldsymbol{Y} - E[\boldsymbol{Y}]][\boldsymbol{Y} - E[\boldsymbol{Y}]]^T\}$$. We denote this by $\boldsymbol{\Sigma}$

We have the following property of the variance-covariance matrix: $$Cov[\boldsymbol{AY}] = \boldsymbol{A\Sigma A^T}$$ where $\boldsymbol{\Sigma}$ is the variance-covariance matrix of $\boldsymbol{Y}$

To prove this, we have that

\begin{equation*}
  \begin{split}
    Cov[\boldsymbol{AY}]
    &= E\{[\boldsymbol{AY - E[AY]}][\boldsymbol{AY - E[AY]}]^T\} \\
    &= E\{[\boldsymbol{AY - AE[Y]}][\boldsymbol{AY - AE[Y]}]^T\} \\
    &= E\{[\boldsymbol{A[Y-E[Y]][Y-E[Y]]^TA^T}\} \\
    &= \boldsymbol{A}E\{\boldsymbol{[Y-E[Y]][Y-E[Y]]^T}\}\boldsymbol{A^T}\\
    &= \boldsymbol{A\Sigma A^T}
  \end{split}
\end{equation*}

## Multivariate Normal Distribution

A random vector $\boldsymbol{Y}$ has a multivariate normal distribution if its density is given by

\begin{equation*}
  f(y_1, \dots, y_n) = \frac{|\boldsymbol{\Sigma}|^{\frac{1}{2}}}{(2\pi)^{\frac{n}{2}}} \cdot exp(-\frac{1}{2}(\boldsymbol{y - \mu})^T \boldsymbol{\Sigma}^{-1} (\boldsymbol{y-\mu}))
\end{equation*}

where $\boldsymbol{y} = [y_1, ..., y_n]^T$, $\boldsymbol{\mu} = [\mu_1, ..., \mu_n]$, and $\boldsymbol{\Sigma} = Cov[\boldsymbol{Y}]$. We denote this by $Y \sim N_n(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. 

If we consider the special case where $n = 1$, we have that $\boldsymbol{\Sigma} = \sigma^2$ and $|\boldsymbol{\Sigma}|^{\frac{1}{2}} = \frac{1}{\sigma}$. Then the density function is 

\begin{equation*}
  f(y_1) = \frac{1}{\sigma \sqrt{2\pi}} \cdot exp(-\frac{1}{2} \frac{(y_1 - \mu_1)^2}{\sigma^2})
\end{equation*}

we get back the univariate normal distribution.

Theorem: Let $\boldsymbol{Y} \sim N_n(\boldsymbol{\mu}, \boldsymbol{\Sigma})$. Let $\boldsymbol{A}$ be an arbitrary $p \times n$ matrix of constants, then we have that $$\boldsymbol{Z} = AY + B \sim N_p(\boldsymbol{A\mu}, \boldsymbol{A\Sigma A^T})$$

Now, if we consider an example where we let $\boldsymbol{Y} \sim N_n(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ and we let $\boldsymbol{A} = [1, ..., 1]^T$, then we have that $$\boldsymbol{AY} \sim N_1(\boldsymbol{A\mu}, A\Sigma A^T)$$ where $\boldsymbol{A\mu} = \sum_{i = 1}^{n} \mu_i, \boldsymbol{A\Sigma A^T} = \sum \sigma_j^2 + 2\sum_{i\neq j} \sigma_{ij}$.

## Matrix Approach to Linear Regression

If we use the matrix representation in regression, it makes it easy to generalize to fitting several independent variables. This would go beyond 1 independent variable. This approach is also known as Multiple Linear Regression.

We use vectors and matrices to denote the observations of the independent variables, the dependent variable, the coefficients, and the random term. 

\begin{itemize}
  \item We let $\boldsymbol{Y} = \begin{bmatrix} Y_1 & \dots & Y_n \end{bmatrix}^T$ be the transpose of the column vector of obervations of the dependent variable
  \item We let $\boldsymbol{\beta} = \begin{bmatrix} \beta_1 & \dots & \beta_n \end{bmatrix}^T$ be the transpose of the column vector of coefficients
  \item We let $\boldsymbol{\epsilon} = \begin{bmatrix} \epsilon_1 & \dots & \epsilon_n \end{bmatrix}^T$ be the transpose of the column vectors of the error terms
  \item We let $\boldsymbol{X} = \begin{pmatrix} 1 & X_{1, 1} & \dots & X_{1, p-1} \\ 1 & X_{2, 1} & \dots &X_{2, p-1} \\ \dots & \dots & \dots & \dots \\ 1 & X_{n, 1} & \dots & X_{n, p-1} \end{pmatrix}$ be the matrix which incorporates the $p-1$ explanatory variables. 
\end{itemize}

If $\boldsymbol{\epsilon} \sim N_n(0, \sigma^2 \boldsymbol{I_n})$, then the regression model may be expressed as $$\boldsymbol{Y} = \boldsymbol{X\beta + \epsilon} \sim N_n(\boldsymbol{X\beta}, \sigma^2 \boldsymbol{I_n})$$ where $\boldsymbol{I_n}$ is the $n \times n$ identity matrix and $N_n$ is the multivariate normal distribution. 

The above is the same as saying that if $\epsilon_i \sim N(0, \sigma^2)$ for $i = 1, ..., n$, then we have that $$Y_i = \beta_0 + \beta_1X_{i, 1} + \beta_2X_{i, 2} + \dots + \beta_{p-1}X_{i, p-1} + \epsilon_i \sim N(\beta_0 + \beta_1X_{i, 1} + \beta_2X_{i, 2} + \dots + \beta_{p-1}X_{i, p-1}, \sigma^2)$$ for $i = 1, ..., n$.

The matrix approach is much nicer because it is more compact and it's can compute more values easily.

## Least Squares Estimations

We want to find an estimate for the vector $\boldsymbol{\beta}$. To do this, we use the least squares approach. However, we're no longer using just scalars. We're instead dealing with vectors and matrices. We need formulas to take derivatives. Below are some facts for taking derivatives in matrix notation.

\begin{itemize}
  \item If $z = \boldsymbol{a}^T\boldsymbol{y}$, then we have $\frac{\partial z}{\partial \boldsymbol{y}} = \boldsymbol{a}$
  \item If $z = \boldsymbol{y}^T\boldsymbol{y}$, then we have $\frac{\partial z}{\partial \boldsymbol{y}} = 2\boldsymbol{y}$
  \item If $z = \boldsymbol{a}^T\boldsymbol{A}\boldsymbol{y}$, then we have $\frac{\partial z}{\partial y} = \boldsymbol{A}^T\boldsymbol{a}$
  \item If $z = \boldsymbol{y}^T\boldsymbol{Ay}$, then we have $\frac{\partial z}{\partial \boldsymbol{y}} = \boldsymbol{A}^T\boldsymbol{y} + \boldsymbol{Ay}$
  \item If $z = \boldsymbol{y}^T\boldsymbol{Ay}$, and $\boldsymbol{A}$ is symmetric, then we have $\frac{\partial z}{\partial \boldsymbol{y}} = 2\boldsymbol{A}^T\boldsymbol{y}$
\end{itemize}

Using the derivative formulas above, we can derive the least squares estimate of the vector $\boldsymbol{\beta}$. To do this, we need to minimize the function 

\begin{equation*}
\begin{split}
  Q 
  &= \boldsymbol{\epsilon}^T\boldsymbol{\epsilon} \\
  &= \sum_{i = 1}^{n} \epsilon_i^2 \\
  &= (\boldsymbol{Y - X\beta})^T(\boldsymbol{Y - X\beta})
\end{split}
\end{equation*}

We can differentiate $Q$ and then obtain the estimate for $\boldsymbol{\beta}$. If we differentiate $Q$, we get $$\frac{\partial Q}{\partial \boldsymbol{\beta}} = -2\boldsymbol{X}^T(\boldsymbol{Y} - \boldsymbol{X\beta})$$. We then set the equation to 0. Then after we solve the equation, we get that a solution for $\boldsymbol{\beta}$ is $\boldsymbol{b} = \boldsymbol{(X^TX)^{-1}X^TY}$.

Therefore, the least squares estimate for $\boldsymbol{\beta}$ is $\boldsymbol{b} = \boldsymbol{(X^TX)^{-1}X^TY}$ if the matrix $(\boldsymbol{X^TX})^{-1}$ exists.

We have that $\boldsymbol{b} = \boldsymbol{(X^TX)^{-1}X^TY}$ is an unbiased estimator of $\boldsymbol{\beta}$. To prove this, we have that

\begin{equation*}
  \begin{split}
    E[\boldsymbol{b}]
    &= \boldsymbol{(X^TX)^{-1}X^TE[Y]} \\
    &= \boldsymbol{(X^TX)^{-1}X^TX\beta} \\
    &= \boldsymbol{\beta}
  \end{split}
\end{equation*}

This means that the least squares estimates of all the parameters are unbiased estimators of their respective parameters.

Now, we want to find the variance-covariance matrix of $\boldsymbol{b}$.

If we let $\boldsymbol{b} = \boldsymbol{AY}$, where $\boldsymbol{A} = \boldsymbol{(X^TX)^{-1}X^T}$. Then we get

\begin{equation*}
\begin{split}
  Cov(\boldsymbol{b}) 
  &= \boldsymbol{A\Sigma A} \\
  &= \sigma^2 \boldsymbol{AA^T} \\
  &= \sigma^2(\boldsymbol{(X^TX)^{-1}(X^TX)(X^TX)^{-1}}) \\
  &= \sigma^2 (\boldsymbol{X^TX})^{-1}
\end{split}
\end{equation*}

We get that $Cov(\boldsymbol{b}) = \sigma^2(\boldsymbol{X^TX})^{-1}$. We have therefore computed the variances of all the least-squares estimates of the parameters and the covariances between them. This is the nice thing about matrix notation, we can compute more values in one shot.

Now that we have computed the expectation and variance of $\boldsymbol{b}$, we can now determine its distribution. We get that $\boldsymbol{b} \sim N_p(\boldsymbol{\beta}, \sigma^2(\boldsymbol{X^TX})^{-1})$.

## The Hat Matrix and its Properties

The predicted value of $\boldsymbol{Y}$ is written as $$\boldsymbol{\hat{Y}} = \boldsymbol{Xb}$$. We can rewrite the equation as $$\boldsymbol{\hat{Y}} = \boldsymbol{HY}$$ where $\boldsymbol{H} = \boldsymbol{X(X^TX)^{-1}X}$. We call $\boldsymbol{H}$ the "hat" matrix.

We have that the hat matrix $\boldsymbol{H}$ is a projection matrix onto the estimation space. It projects $\boldsymbol{Y}$ onto the estimation space, leading to $\boldsymbol{\hat{Y}} = \boldsymbol{HY}$. The hat matrix is also idempotent. To show this, we have that

\begin{equation*}
\begin{split}
  \boldsymbol{HH}
  &= \boldsymbol{X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T} \\
  &= \boldsymbol{XI_n(X^TX)^{-1}X^T}\\
  &= \boldsymbol{X(X^TX)^{-1}X^T} \\
  &= \boldsymbol{H}
\end{split}
\end{equation*}

The hat matrix is also symmetric, which means that $\boldsymbol{H^T} = \boldsymbol{H}$. To show this, we have

\begin{equation*}
\begin{split}
  \boldsymbol{H^T}
  &= \boldsymbol{(X(X^TX)^{-1}X^T)^T} \\
  &= \boldsymbol{X(X^TX)^{-1}X^T} \\
  &= \boldsymbol{H}
\end{split}
\end{equation*}

We also have that the matrix $\boldsymbol{(I-H)}$ is idempotent ($\boldsymbol{I}$ is the identity matrix). To show this, we have

\begin{equation*}
\begin{split}
  \boldsymbol{(I-H)(I-H)}
  &= \boldsymbol{II - IH - HI + HH} \\
  &= \boldsymbol{I - H - H + H} \\
  &= \boldsymbol{I - H}
\end{split}
\end{equation*}

We have that the matrix $\boldsymbol{H}$ and the matrix $\boldsymbol{I-H}$ are orthogonal. To show this, we have

\begin{equation*}
\begin{split}
  \boldsymbol{H(I-H)}
  &= \boldsymbol{HI - HH} \\
  &= \boldsymbol{H - H} \\
  &= \boldsymbol{0}
\end{split}
\end{equation*}

We can express the residual vector as $\boldsymbol{e} = \boldsymbol{(I-H)Y}$. To show this, we have

\begin{equation*}
\begin{split}
  \boldsymbol{e}
  &= \boldsymbol{Y - \hat{Y}} \\
  &= \boldsymbol{Y - HY} \\
  &= \boldsymbol{(I-H)Y}
\end{split}
\end{equation*}

Putting all the properties together, we have that $\boldsymbol{\hat{Y}} = \boldsymbol{HY}$, $\boldsymbol{e} = \boldsymbol{(I-H)Y}$, and $\boldsymbol{Y} = \boldsymbol{HY + (I-H)Y}$. We get that by the Pythagorean theorem, we have $$||\boldsymbol{Y}||^2 = ||\boldsymbol{HY}||^2 + ||\boldsymbol{(I-H)Y}||^2$$

We also get that $Cov[\boldsymbol{e}] = \sigma^2(\boldsymbol{I-H})$, which is estimated by $s^2[\boldsymbol{e}] = MSE(\boldsymbol{I-H})$.

Now, we want to consider the special case where $p = 2$. This is the case with 1 predictor variable, which goes back to simple linear regression. We want to compute the hat matrix for this case.

We let $\boldsymbol{X} = \begin{pmatrix} 1 & (X_1 - \overline{X}) \\ \dots & \dots \\ 1 & (X_n - \overline{X}) \end{pmatrix}$. Then we have that $\boldsymbol{X^TX} = \begin{pmatrix} n & 0 \\ 0 & \sum (X_i - \overline{X})^2 \end{pmatrix}$. Then we get that $\boldsymbol{(X^TX)^{-1}} = \begin{pmatrix} \sum (X_i - \overline{X})^2 & 0 \\ 0 & n \end{pmatrix} \frac{1}{n \sum (X_i - \overline{X})}$. 

Now, we can compute the hat matrix.

\begin{equation*}
\begin{split}
  \boldsymbol{H}
  &= \boldsymbol{(X^TX)^{-1}X^T} \\
  &= \begin{pmatrix} \sum (X_i - \overline{X})^2 + n(X_1 - \overline{X})^2 & \dots & \sum (X_i - \overline{X})^2 + n(X_1 - \overline{X})^2 + n(X_1 - \overline{X})(X_n - \overline{X}) \\ \dots & \dots & \dots \\ \sum(X_i - \overline{X})^2 + n(X_1 - \overline{X})(X_n - \overline{X}) & \dots & \sum (X_i - \overline{X})^2 + n(X_n - \overline{X})^2 \end{pmatrix} \cdot \frac{1}{n\sum (X_i - \overline{X})^2}\\
  &= \begin{pmatrix} \frac{1}{n} & \dots & \frac{1}{n} \\ \dots & \dots & \dots \\ \frac{1}{n} & \dots & \frac{1}{n} \end{pmatrix} + \begin{bmatrix} X_1 - \overline{X} \\ \dots \\ X_n - \overline{X} \end{bmatrix} \begin{bmatrix} X_1 - \overline{X} & \dots & X_n - \overline{X} \end{bmatrix} \frac{1}{\sum (X_i - \overline{X})^2} \\
  &= \frac{1}{n} \boldsymbol{J} + \begin{bmatrix} X_1 - \overline{X} \\ \dots \\ X_n - \overline{X} \end{bmatrix} \begin{bmatrix} k_1 & \dots & k_n \end{bmatrix}\\
\end{split}
\end{equation*}

Note: $\boldsymbol{J}$ is a matrix of 1s.

Now that we have computed the hat matrix for 2 predictor variables, we can compute the least squares regression line in matrix form.

\begin{equation*}
\begin{split}
  \hat{\boldsymbol{Y}}
  &= \boldsymbol{HY} \\
  &= \frac{1}{n}\boldsymbol{JY} + \begin{bmatrix} X_1 - \overline{X} \\ \dots \\ X_n - \overline{X} \end{bmatrix} \begin{bmatrix} k_1 & \dots & k_n \end{bmatrix} \boldsymbol{Y} \\
  &= \begin{bmatrix} \overline{Y} \\ \dots \\ \overline{Y} \end{bmatrix} + \begin{bmatrix} X_1 - \overline{X} \\ \dots \\ X_n - \overline{X} \end{bmatrix} b_1 \\
  &= \overline{Y}\boldsymbol{1_n} + b_1 \begin{bmatrix} X_1 - \overline{X} \\ \dots \\ X_n - \overline{X} \end{bmatrix}
\end{split}   
\end{equation*}

Now, we can find the trace and rank of the hat matrix $\boldsymbol{H}$ and we show that it is equal to 2.

\begin{equation*}
  \begin{split}
  Rank(\boldsymbol{H})
  &= Trace(\boldsymbol{H}) \\
  &= \frac{n\sum(X_i - \overline{X})^2 + n\sum (X_i - \overline{X})^2}{n\sum (X_i - \overline{X})^2} \\
  &= 2
  \end{split}
\end{equation*}

## Quadratic Forms

We now want to look at the theory behind the relationship between sums of squares. We first need to look at a fundamental concept.

If we let $Y_1, ..., Y_n$ be a random sample from $N(\mu, \sigma^2)$. A quadratic form in the $Y$'s is defined to be the real quantity $\boldsymbol{Q} = \boldsymbol{Y^TAY}$, where $\boldsymbol{A}$ is a symmetric positive definite matrix. The singular decomposition of $\boldsymbol{A}$ implies that there exists an orthogonal matrix $\boldsymbol{P}$ such that if $\boldsymbol{\Lambda} = (\lambda_i)$ is the diagonal matrix of eigenvalues of $\boldsymbol{A}$, we have $\boldsymbol{A} = \boldsymbol{P^T\Lambda P}$.

Proportion: $E[\boldsymbol{Y^TAY}] = Trace[\boldsymbol{A\Sigma}] + E[\boldsymbol{Y}]^T\boldsymbol{A}E[\boldsymbol{Y}]$.

To show this, we have

\begin{equation*}
\begin{split}
  \boldsymbol{Y^TAY}
  &= \boldsymbol{Y^TP^T\Lambda PY} \\
  &= \boldsymbol{(PY)^T\Lambda (PY)} \\
  &= \sum \lambda_i ||(\boldsymbol{PY})_i||^2 \\
\end{split}
\end{equation*}

where $(\boldsymbol{PY})_i$ is the i-th element in the vector $PY$. The second moment of $(\boldsymbol{PY})_i$ is

\begin{equation*}
\begin{split}
  E[||(\boldsymbol{PY})_i||^2]
  &= Var[||(\boldsymbol{PY})_i||] + (E[(\boldsymbol{PY})_i])^2 \\
  &= (\boldsymbol{P\Sigma P^T})_{ii} + [(\boldsymbol{PE[Y]})_i]^2
\end{split}
\end{equation*}

Now, we get

\begin{equation*}
\begin{split}
  E[\sum \lambda_i ||(PY)_i||^2] 
  &= \sum \lambda_i (\boldsymbol{P\Sigma P^T})_{ii} + \sum \lambda_i [(\boldsymbol{PE[Y]})_i]^2 \\
  &= Trace(\Lambda \boldsymbol{P \Sigma P^T}) + \boldsymbol{\mu^T A \mu} \\
  &= Trace(\boldsymbol{P^T \Lambda P \Sigma}) + \boldsymbol{\mu^T A \mu} \\
  &= Trace(\boldsymbol{A\Sigma}) + \boldsymbol{\mu^T A \mu}
\end{split}
\end{equation*}

Lemma: The mean squared error is an unbiased estimate of $\sigma^2$.

To prove this, we have that the residual sum of squares (SSE) is 

$$\sum e_i^2 = \sum(Y_i - \hat{Y})^2$$

This can be written in matrix notation as

$$(\boldsymbol{Y - \hat{Y}})^T(\boldsymbol{Y - \hat{Y}})$$

We also know for a fact that $\boldsymbol{Y - \hat{Y}} = \boldsymbol{(I - H)Y}$ and $\boldsymbol{I-H}$ is idempotent. We get that

$$(\boldsymbol{Y - \hat{Y}})^T(\boldsymbol{Y - \hat{Y}}) = \boldsymbol{Y^T(I-H)Y}$$
Then we have that

\begin{equation*}
  \begin{split}
    E[\boldsymbol{Y^T(I-H)Y}]
    &= Trace(\boldsymbol{(I-H)\Sigma}) + \boldsymbol{\mu^T(I-H)\mu} \\
    &= \sigma^2 Trace(\boldsymbol{I - H}) + (\boldsymbol{X\beta})^T(\boldsymbol{I - H})(\boldsymbol{X\beta}) \\
    &= \sigma^2(n-p) + \boldsymbol{\beta^TX^T}(\boldsymbol{I - X(X^TX)^{-1}X^T})\boldsymbol{X\beta} \\
    &= \sigma^2(n-p) + \boldsymbol{\beta^T(X^T - X^TX(X^TX)^{-1}X^T)X\beta} \\
    &= \sigma^2(n-p) + \boldsymbol{\beta^T(X^T-X^T)X\beta} \\
    &= \sigma^2(n-p) + 0 \\
    &= \sigma^2(n-p)
  \end{split}
\end{equation*}

Consequently, we get that

\begin{equation*}
\begin{split}
  E[MSE]
  &= E[\frac{SSE}{n-p}] \\
  &= \frac{E[SSE]}{n-p} \\
  &= \frac{\sigma^2(n-p)}{n-p} \\
  &= \sigma^2
\end{split}
\end{equation*}

## Chi-Squared distribution and F distribution

A random variable $U$ has a chi-squared $\chi^2_{\nu}$ distribution with $\nu$ degrees of freedom if its density is given by

$$f(u; \nu) = \frac{1}{2^{\frac{\nu}{2}} \Gamma(\nu/2)} u^{(\nu/2)-1}e^{-u/2}$$ for $u>0, \nu > 0$. The mean of $U$ is $\nu$ and the variance of $U$ is $2\nu$.

A random variable $U$ has a non-central chi-squared distribution $\chi^2_{\nu}(\lambda)$ with $\nu$ degrees of freedom and non-centrality parameter $\lambda$ if its density is given by

$$f(u; \nu, \lambda) = \sum_{i=0}^{\infty} e^{-\lambda/2} \frac{(\lambda/2)^i}{i!}f(u; \nu + 2i)$$ with $u>0, \nu>0$. The mean of $U$ is $\nu + \lambda$ and the variance of $U$ is $2\nu + 4\lambda$.

If we let $U_1 \sim \chi^2_{\nu_1}$ and $U_2 \sim \chi^2_{\nu_2}$, then we have that $$F = \frac{U_1/\nu_1}{U_2/\nu_2} \sim F(\nu_1, \nu_2)$$ If the numerator has a non-central chi-squared distribution, then F has a non-central F distribution.

## Cochran's Theorem

Cochran's Theorem states that if we let $Y$ be a random vector with a multivariate normal distribution $N_n(\boldsymbol{\mu}, \sigma^2 \boldsymbol{I})$ and suppose that we have the decomposition $$\boldsymbol{Y^TY} = Q_1 + \dots + Q_k$$ where $Q_i = \boldsymbol{Y^TA_iY}$ and $rank(A_i) = n_i$. Then $\frac{Q_i}{\sigma^2}$ are independent and have a non-central chi-squared distribution with $n_i$ degrees of freedom and non-centrality parameter $\lambda_i$, where $\lambda_i = \boldsymbol{\mu^T A_i \mu}$.

We have some examples of quadratic forms that are particularly important for analysis.

We let $\boldsymbol{Y} \sim N_n(\boldsymbol{\mu}, \sigma^2\boldsymbol{I_n})$ be the response vector. We can decompose $\boldsymbol{Y^TY}$ the following way:

$$\boldsymbol{Y^TY} = \boldsymbol{Y^TAY} + \boldsymbol{Y^T}\frac{\boldsymbol{1_n1_n^T}}{n}\boldsymbol{Y}$$ 
where $\boldsymbol{A} = \begin{pmatrix} 1-\frac{1}{n} & -\frac{1}{n} & \dots & -\frac{1}{n} \\ -\frac{1}{n} & 1 - \frac{1}{n} & \dots & -\frac{1}{n} \\ \dots & \dots & \dots & \dots \\ -\frac{1}{n} & -\frac{1}{n} & \dots & 1-\frac{1}{n} \end{pmatrix}$ (an $n \times n$ matrix with $1-\frac{1}{n}$ on the diagonals and $-\frac{1}{n}$ on the off-diagonals) and $\boldsymbol{1_n} = \begin{bmatrix} 1 \\ 1 \\ \dots \\ 1 \end{bmatrix}$ (n-dimensional column vector with all 1s). 

We can rewrite $\boldsymbol{Y^TAY}$ the following way:

\begin{equation*}
\begin{split}
  \boldsymbol{Y^TAY}
  &= \begin{bmatrix} Y_1 & \dots & Y_n \end{bmatrix} \begin{pmatrix} 1 - \frac{1}{n} & -\frac{1}{n} & \dots & \frac{1}{n} \\ -\frac{1}{n} & 1 - \frac{1}{n} & \dots & -\frac{1}{n} \\ \dots & \dots & \dots & \dots \\ -\frac{1}{n} & -\frac{1}{n} & \dots & 1 - \frac{1}{n} \end{pmatrix} \begin{bmatrix} Y_1 \\ Y_2 \\ \dots \\ Y_n \end{bmatrix} \\
  &= \begin{bmatrix} Y_1 - \overline{Y} & Y_2 - \overline{Y} & \dots & Y_n - \overline{Y} \end{bmatrix} \begin{bmatrix} Y_1 \\ Y_2 \\ \dots \\ Y_n \end{bmatrix} \\
  &= Y_1(Y_1 - \overline{Y}) + Y_2(Y_2 - \overline{Y}) + \dots + Y_n(Y_n - \overline{Y}) \\
  &= \sum Y_i(Y_i - \overline{Y}) \\
  &= \sum Y_i^2 - \overline{Y} \sum Y_i \\
  &= \sum Y_i^2 - n\overline{Y} \\
  &= \sum (Y_i - \overline{Y})^2
\end{split}
\end{equation*}

We can also rewrite $\boldsymbol{Y}^T\frac{\boldsymbol{1_n^T1_n}}{n}\boldsymbol{Y}$ the following way:

\begin{equation*}
\begin{split}
  \boldsymbol{Y}^T \frac{\boldsymbol{1_n^T1_n}}{n} \boldsymbol{Y} 
  &= \boldsymbol{Y}^T \frac{\boldsymbol{J_n}}{n} \boldsymbol{Y} \\
  &= \begin{bmatrix} Y_1 & Y_2 & \dots & Y_n \end{bmatrix} \begin{pmatrix} \frac{1}{n} & \frac{1}{n} & \dots & \frac{1}{n} \\ \frac{1}{n} & \frac{1}{n} & \dots & \frac{1}{n} \\ \dots & \dots & \dots & \dots \\ \frac{1}{n} & \frac{1}{n} & \dots & \frac{1}{n} \end{pmatrix} \begin{bmatrix} Y_1 \\ Y_2 \\ \dots \\ Y_n \end{bmatrix} \\
  &= \begin{bmatrix} \overline{Y} & \overline{Y} & \dots & \overline{Y} \end{bmatrix} \begin{bmatrix} Y_1 \\ Y_2 \\ \dots \\ Y_n \end{bmatrix} \\
  &= n\overline{Y}
\end{split}
\end{equation*}

So now we get that $\boldsymbol{Y^TY} = \sum (Y_i - \overline{Y})^2 + n\overline{Y}$. We can now look at the degrees of freedom of $\sum (Y_i - \overline{Y})^2$ and $n\overline{Y}$.

We get that $\sum (Y_i - \overline{Y})^2 = \boldsymbol{Y^TAY}$, where $\boldsymbol{A} = \begin{pmatrix} 1-\frac{1}{n} & -\frac{1}{n} & \dots & -\frac{1}{n} \\ -\frac{1}{n} & 1-\frac{1}{n} & \dots & -\frac{1}{n} \\ \dots & \dots & \dots & \dots \\ -\frac{1}{n} & -\frac{1}{n} & \dots & 1 - \frac{1}{n} \end{pmatrix}$. We know that $A$ is idempotent and symmetric. Then we get that $rank(A) = trace(A) = n(1 - \frac{1}{n}) = n - 1$. This explains why $\frac{\sum (Y_i - \overline{Y})^2}{\sigma^2}$ has a chi-squared distribution with $n-1$ degrees of freedom.

We also know that $\frac{\boldsymbol{1_n1_n^T}}{n}$ is an $n \times n$ matrix with $\frac{1}{n}$ as all its entries. This makes it an idempotent and symmetric matrix. It has $rank = trace = n(\frac{1}{n}) = 1$.

Therefore, we get that the ranks sum up to $n$ and we have proven that $\frac{\sum(Y_i - \overline{Y})^2}{\sigma^2}$ has a chi-squared distribution with $n-1$ degrees of freedom.

# Multiple Linear Regression

## Linear models with 2 or more predictors

We usually see models with 2 or more predictor variables rather than 1 in the case of simple linear regression. For instance, with 2 predictors, we have models in the form $$Y_i = \beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon_i$$ with $\epsilon_i \sim N(0, \sigma^2)$. This model displays a plane in 3 dimensions and $\beta_1$ represents the rate of change in a unit increase in $X_1$ when $X_2$ is fixed and vice versa for $\beta_2$.

In a model with $p - 1$ predictors, we have that the model is in the form $$Y_i = \beta_0 + \sum_{i=1}^{p-1}\beta_kX_{ik} + \epsilon_i$$ where $\beta_k$ is the rate of change in a unit increase in $X_k$ when all other explanatory variables are held fixed.

## Matrix Approach: Review

To make the equation of the linear model more compact, we use the matrix notation discussed in section 2. 

\begin{itemize}
  \item We let $\boldsymbol{Y} = \begin{bmatrix} Y_1 & \dots & Y_n \end{bmatrix}^T$ be the transpose of the column vector of obervations of the dependent variable
  \item We let $\boldsymbol{\beta} = \begin{bmatrix} \beta_1 & \dots & \beta_n \end{bmatrix}^T$ be the transpose of the column vector of coefficients
  \item We let $\boldsymbol{\epsilon} = \begin{bmatrix} \epsilon_1 & \dots & \epsilon_n \end{bmatrix}^T$ be the transpose of the column vectors of the error terms
  \item We let $\boldsymbol{X} = \begin{pmatrix} 1 & X_{1, 1} & \dots & X_{1, p-1} \\ 1 & X_{2, 1} & \dots &X_{2, p-1} \\ \dots & \dots & \dots & \dots \\ 1 & X_{n, 1} & \dots & X_{n, p-1} \end{pmatrix}$ be the matrix which incorporates the $p-1$ explanatory variables. 
\end{itemize}

Then we can write the model as $$\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}$$ with $\boldsymbol{\epsilon} \sim N_n(0, \sigma^2\boldsymbol{I_n})$.

Recall that the least squares estimator for $\boldsymbol{\beta}$ is $\boldsymbol{b} = \boldsymbol{(X^TX)^{-1}X^TY}$ and the fitted values are $\boldsymbol{\hat{Y}} = \boldsymbol{Xb} = \boldsymbol{HY}$, where $\boldsymbol{H} = \boldsymbol{X(X^TX)^{-1}X^T}$.

We also recall that the variance-covariance matrix of the residuals is $Cov(\boldsymbol{e}) = \sigma^2(\boldsymbol{I-H})$, which can be estimated by $s^2[\boldsymbol{e}] = (MSE)(\boldsymbol{I-H})$. We also have $s^2[\boldsymbol{b}] = (MSE)(\boldsymbol{X^TX})^{-1}$.

We can perform ANOVA on multiple regression models. To do this, it is similar to simple linear regression, except that we need to use matrix notation to write the sums of squares. We have that

\begin{itemize}
  \item $SSTO = \boldsymbol{Y^TY} - \frac{1}{n}\boldsymbol{Y^TJY} = \boldsymbol{Y^T}(\boldsymbol{I} - \frac{1}{n}\boldsymbol{J})\boldsymbol{Y}$
  \item $SSE = \boldsymbol{e^Te} = \boldsymbol{Y^T(I-H)Y}$
  \item $SSR = \boldsymbol{b^TX^TY} - \frac{1}{n}\boldsymbol{Y^TJY} = \boldsymbol{Y^T}(\boldsymbol{H} - \frac{1}{n}\boldsymbol{J})\boldsymbol{Y}$
\end{itemize}

These values are all in quadratic form. We get that $SSTO = SSR + SSE$. By Cochran's theorem, we get that SSR, SSE, and SSTO have a chi-squared distribution with degrees of freedom $p-1$, $n-p$, and $n-1$, respectively. This can be shown by computing the ranks of the matrices $\boldsymbol{H - \frac{1}{n}J}$ and $\boldsymbol{I-H}$. The ANOVA table is the same as for simple linear regression except that SSR has degree of freedom $p - 1$ and SSE has degree of freedom $n - p$. This is not restricted to $p = 2$. Below is the typical structure of an ANOVA table (this is the same as for simple linear regression). 

\begin{table}[!h]
\centering
\caption{ANOVA Table}
\begin{tabular}{|c|c|c|c|c|c|}

\hline
Source & Sum of Squares (SS) & df & Mean Square (MS = SS/df) & F statistic & E[MS] \\
\hline
Regression & $SSR = b_1^2\sum(X_i - \overline{X})^2$ & $p - 1$ & $MSR = \frac{SSR}{p-1}$ & $\frac{MSR}{MSE}$ & $\sigma^2 + \beta_1^2 \sum(X_i - \overline{X})^2$ \\
\hline
Error & $SSE = \sum (Y_i - \hat{Y_i})^2$ & $n - p$ & $MSE = \frac{SSE}{n-p}$ & & $\sigma^2$\\
\hline
& & & & & \\
\hline
Total & $SSTO = \sum(Y_i - \overline{Y})^2$ & $n - 1$ & & &\\
\hline
  
\end{tabular}
\label{lyxtab2}
\end{table}

We can use this to test the null hypothesis $H_0: \beta_1 = \beta_2 = \dots = \beta_{p-1} = 0$ against $H_1: \text{not all } \beta_k = 0$. We do this by looking at the F statistic $F = \frac{MSR}{MSE}$ and we reject $H_0$ for large values of F.

We can also do hypothesis tests for individual coefficients. Say we test $H_0: \beta_k = 0$ against $H_1: \beta_k \neq 0$, then we can compute the test statistic $$t = \frac{b_k}{s[b_k]} \sim t_{n-p}$$ and we reject $H_0$ for large values of t. ($s^2[b_k] = MSE(\boldsymbol{X^TX})_{kk}^{-1}$).

## Extra Sums of Squares Principle 

We can use a more general approach to regression to test if we can fit a reduced model rather than a full model to model the data. We first illustrate the case for $p = 2$.

To do this, we let the full model (F) be the model $Y = \beta_0 + \beta_1X + \epsilon$ and the reduced model (R) be the model $Y = \beta_0 + \epsilon$. We compute the error sum of squares for each model. We get that $SSE(F) = \sum (Y_i - \hat{Y_i})^2$ for the full model and $SSE(R) = \sum(Y_i - \hat{Y_i})^2$ for the reduced model. This way we can test $H_0: \beta_1 = 0$ against $H_1: \beta_1 \neq 0$ by computing the following statistic: $$F^* = \frac{\frac{SSE(R)-SSE(F)}{df_R - df_F}}{\frac{SSE(F)}{df_F}}$$
We reject $H_0$ for large values of $F^*$, which has an $F$ distribution with degrees of freedom $df_R - df_F$ and $df_F$, respectively. In other words, $F^* \sim F(df_R - df_F, df_F)$

An immediate application of this approach is to the situation where there are repeat observations at the same values of $X$ (i.e. when there are multiple observed $Y$ values at the same $X$ value). Suppose that the full model is given by $$Y_{ij} = \mu_j + \epsilon_{ij}$$ where $i = 1, \dots, n_j$ and $j = 1, \dots, c$ and $\epsilon_{ij} \sum N(0, \sigma^2)$. The $\mu_{j}$ values are unrestricted parameters when $X = X_j$. To derive their least squares estimates, we want to minimize the following: $Q = \sum_{j=1}^{c}\sum_{i=1}^{n_j} \epsilon_{ij}^2 = \sum_{j=1}^{c}\sum_{i=1}^{n_j}[Y_{ij} - \mu_j]^2$. We take the derivative and we get

\begin{equation*}
\begin{split}
  \frac{\partial Q}{\partial \mu_j}
  &= \frac{\partial \sum_{i=1}^{n_j} (Y_{ij} - \mu_j)^2}{\partial \mu_j} \\
  &= -2 \sum_{i=1}^{n_j}(Y_{ij} - \mu_j)
\end{split}
\end{equation*}

We set the above equal to 0. Then we get that 

\begin{equation*}
\begin{split}
  -2 \sum_{i=1}^{n_j} (Y_{ij} - \mu_j) = 0 \\
  \sum_{i=1}^{n_j} Y_{ij} - \sum_{i=1}^{n_j} \mu_j = 0\\
  n_j\mu_j = \sum_{i=1}^{n_j}Y_{ij} \\
  \hat{\mu} = \frac{\sum_{i=1}^{n_j} Y_{ij}}{n_j}
\end{split}
\end{equation*}

So we have that the least squares estimators of $\mu_j$ are $$\overline{Y_j} = \frac{\sum_{i=1}^{n_j} Y_{ij}}{n_j}$$

Therefore, the error sum of squares for this full unrestricted model is $$SSE(F) = \sum_{ij} (Y_{ij} - \overline{Y_j})^2$$

The corresponding degrees of freedom are $$df_F = \sum_{j=1}^{c} (n_j - 1) = n-c$$

If all $n_j = 1$, then $df_F = 0$ and $SSE(F) = 0$, and the analysis cannot proceed any further.

Now, we have the reduced model $$Y_{ij} = \beta_0 + \beta_1X_j + \epsilon_{ij}$$

which has error sum of squares equal to $$SSE(R) = \sum_{ij} (Y_{ij} - \hat{Y_{ij}})^2$$ where $\hat{Y_{ij}} = b_0 + b_1X_j$. The degrees of freedom are $df_R = (n-2)$

Now, we can test the hypotheses $$H_0: E[Y] = \beta_0 + \beta_1X$$ $$H_1: E[Y] \neq \beta_0 + \beta_1X$$ by computing the ratio $$F^* = \frac{[\frac{SSE(R)-SSE(F)}{df_R - df_F}]}{[\frac{SSE(F)}{df_F}]}$$ The test is on whether a linear model is justified at all. This is different from just testing that the slope is 0. The main purpose of this test is to see if we can use a linear model instead of a complex model.

We can gain some insight into the components of the $F^*$ ratio. We have that $$(Y_{ij} - \hat{Y_{ij}}) = (Y_{ij} - \overline{Y_j}) - (\overline{Y_j} - \hat{Y_{ij}})$$ We then get the relationship $$\sum_{ij}(Y_{ij} - \hat{Y_{ij}})^2 = \sum_{ij} (Y_{ij} - \overline{Y_j})^2 + \sum_{ij} (\overline{Y_j} - \hat{Y_{ij}})^2$$

The components are broken down as follows:

\begin{itemize}
  \item $SSE(R) = \sum_{ij} (Y_{ij} - \hat{Y_{ij}})^2$ is the error sum of squares for the reduced model
  \item $SSPE = \sum_{ij} (Y_{ij} - \overline{Y_j})^2$ is the pure error sum of squares
  \item $SSLF = \sum_{ij} (\overline{Y_j} - \hat{Y_{ij}})^2$ is the error sum of squares due to lack of fit which is independent of $i$
\end{itemize}

We also have that the the degrees of freedom of the pure error sum of squares is $df_{PE} = n-c$ and the degrees of freedom of the lack of fit sums of squares is $df_{LF} = c-2$. An ANOVA table summarizes the analysis:

\begin{table}[!h]
\centering
\caption{ANOVA Table for Lack of Fit Test}
\begin{tabular}{|c|c|c|c|c|c|}

\hline
Source & Sum of Squares (SS) & df & Mean Square (MS = SS/df) & F statistic & E[MS] \\
\hline
Regression & $SSR = \sum_{ij}(\hat{Y_{ij}} - \overline{Y})^2$ & $1$ & $MSR = SSR$ & $\frac{MSR}{MSE}$ & $\sigma^2 + \beta_1^2 \sum(X_i - \overline{X})^2$ \\
\hline
Error & $SSE(R) = \sum (Y_{ij} - \hat{Y_{ij}})^2$ & $n - 2$ & $MSE = \frac{SSE(R)}{n-2}$ & & $\sigma^2$\\
\hline
Lack of Fit & $SSLF = \sum_{ij}(\overline{Y_j} - \hat{Y_{ij}})^2$ & $c-2$ & $MSLF = \frac{SSLF}{c-2}$ & $F^* = \frac{MSLF}{MSPE}$ & $\sigma^2 + \frac{\sum n_i(\mu_i - \beta_0 - \beta_1X_i)^2}{c-2}$\\
\hline
Pure Error & $SSPE = \sum_{ij}(Y_{ij} - \overline{Y_j})^2$ & $n-c$& $MSPE = \frac{SSPE}{n-c}$& & \\
\hline
&&&&&\\
\hline
Total & $SSTO = \sum(Y_i - \overline{Y})^2$ & $n - 1$ & & &\\
\hline
  
\end{tabular}
\label{lyxtab2}
\end{table}

The $F^*$ ratio tests for lack of fit with a simple linear regression model. If there is no lack of fit, the the ratio should be closer to 1 since both the pure error sums of squares and the error sum of squares due to lack of fit are unbiased estimators of $\sigma^2$ under $H_0$. Otherwise, we would expect the ratio to be large.

We can also extend this concept to multiple linear regression. We consider the case where we have 2 predictors.

We define $$SSR(X_2|X_1) = SSE(X_1) -SSE(X_1, X_2)$$ to be the reduction in the error sum of squares when after $X_1$ is included, an additional variable $X_2$ is added to the model. We can rewrite the expression as $$SSR(X_2|X_1) = SSR(X_1, X_2) - SSR(X_1)$$

Similarly, when we have 3 predictors, we have that $$SSR(X_1, X_2, X_3) = SSR(X_1) + SSR(X_2|X_1) + SSR(X_3|X_1, X_2)$$ This decomposition enables us to judge the effect an added variable has on the sum of squares due to regression. 

We can use this process to test a full model with all predictors and a reduced model with only selected predictors by obtaining the error sum of squares of the full and reduced models.

## Example: Delivery Time Data

A soft drink bottler is interested in predicting the time required by the route driver to deliver the vending machines in an outlet. We let $Y$ be the delivery time, $X_1$ be the number of cases of product stocked, and $X_2$ be the distance walked by the route driver in feet. 

We first want to create a scatterplot matrix of the data

```{r}
delivery = read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Delivery Time.txt", header =TRUE, sep='\t')
names(delivery)
plot(delivery)
```
Now, we want to fit a multiple linear regression model for the data.

```{r}
X_1 = delivery$Number.of.Cases
X_2 = delivery$Distance
Y = delivery$Delivery.Time
model = lm(Y~X_1+X_2, data=delivery)
model
summary(model)
```
From the summary above, we find that the least squares multiple linear regression function is $\hat{Y} = 2.341231 + 1.615907X_1 + 0.014385X_2$. If we want to test for regression, we see that the $F$ statistic is 261.2 and the p-value is very small. This means that we can reject $H_0: \beta_1 = \beta_2 = 0$, which means that there is enough evidence to suggest that at least one of $X_1$ and $X_2$ have influence on $Y$. If we look at the t statistics, we have that we can reject the null hypotheses $H_0: \beta_0 = 0$, $H_0: \beta_1 = 0$, and $H_0: \beta_2 = 0$. This is because the t statistics are all statistically significant.

We can also do an ANOVA test on the model

```{r}
anova(model)
```
We have found that the $F$ statistic to test $H_0: \beta_1 = 0$ is 506.619 and the $F$ statistic to test $H_0: \beta_2 = 0$ is 15.851, which are both significant. There is convincing evidence that both $X_1$ and $X_2$ have influence on $Y$.

We can also conduct a test using the extra sum of squares principle. We're testing the full model $Y = \beta_0 + \beta_1X_1 + \beta_2X_2$ against the reduced model $Y = \beta_0 + \beta_1X_1$.

```{r}
Full = lm(Y~X_1+X_2, data=delivery)
Reduced = lm(Y~X_1)
anova(Reduced, Full)
```
We find that the $F$ statistic is 15.851, which is statistically significant. Therefore, we can reject the null hypothesis that $H_0: \beta_2 = 0$. This means that we cannot use the reduced model for this data.

## Example: Bank Data

We want to illustrate for testing for lack of fit. For this, we'll use the Bank data. We'll compare the reduced model $Y_{ij} = \beta_0 + \beta_1X_j + \epsilon_{ij}$ and the full model $Y_{ij} = \mu_j + \epsilon_{ij}$ for a data with repeat observation at the same predictor values. 

```{r}
#We first load the data
bank = read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Bank Data.txt", header=TRUE, sep='\t')
names(bank)

#Now, we fit the reduced and full models for the data
x = bank$Minimum.Deposit
y = bank$Number.New.accounts

reduced = lm(y~x)
full = lm(y~0 + as.factor(x))
anova(reduced, full)
```

The $F$ statistic is 14.801, which is statistically significant. Therefore, we reject the null hypothesis that $E[Y] = \beta_0 + \beta_1X$ and a linear model is not a good fit for the data.

## Simultaneous Confidence Intervals

We have learned to construct a confidence interval for one specific parameter (i.e. confidence intervals for $\beta_0$ and $\beta_1$ in a simple linear regression model). However, sometimes we want to calculate simultaneous or joint confidence intervals for the entire set of parameters. For example, we may want to construct simultaneous confidence intervals for all the coefficients in a linear regression model (i.e. simultaneous confidence intervals that contain both the intercept and slope in a simple linear regression model). However, the confidence level decreases as we include more parameters to estimate.

For example, we consider 2 parameters: $\beta_0$ and $\beta_1$ of a simple linear regression model, and we want to construct simultaneous $100(1-\alpha/2)\%$ confidence intervals for the parameters. We can obtain a $100(1-\alpha/2)\%$ for each parameter. However, if we let $A_1$ be the event that $\beta_0$ is in its confidence interval and $A_2$ be the event that $\beta_1$ is in its confidence interval, then if we assume that $A_1$ and $A_2$ are independent, then we have that $$P(A_1 \cap A_2) = P(A_1)P(A_2)$$ Say if we have $P(A_1) = 0.95$ and $P(A_2) = 0.95$, then $P(A \cap B) = (0.95)^2 = 0.9025 < 0.95$. However, the events are never independent, so $P(A_1 \cap A_2)$ is even less.

One strategy is to use the Bonferroni's procedure. Bonferroni's inequality states that for 2 events $\overline{A_1}$, $\overline{A_2}$, we have that $$P(\overline{A_1} \cap \overline{A_2}) = P(\overline{A_1}) + P(\overline{A_2}) - P(\overline{A_1} \cap \overline{A_2}) \le P(\overline{A_1}) + P(\overline{A_2})$$ and then we use DeMorgan's identity: $$P(A_1 \cap A_2) = 1 - P(\overline{A_1} \cup \overline{A_2}) \ge 1 - P(\overline{A_1}) - P(\overline{A_2})$$

We define $A_1$ as the event that $\beta_0$ is contained in its $100(1-\alpha)\%$ confidence interval and $A_2$ is the event that $\beta_1$ is contained in its $100(1-\alpha)\%$ confidence interval. In this case, we have that $$P(\overline{A_1}) = P(\overline{A_2}) = \alpha$$ and hence, we get that $$P(A_1 \cap A_2) \ge 1 - P(A_1) - P(A_2) \ge 1 - 2\alpha$$

Now the event $A_1 \cap A_2$ is the event that the intervals $b_0 \pm t(\alpha/2; n-2)\cdot s[b_0]$ and $b_1 \pm t(\alpha/2; n-2)\cdot s[b_1]$ simultaneously cover $\beta_0$ and $\beta_1$, respectively. The probability of such event is $1 - 2\alpha$. If we have $\alpha = 0.05$, then we get that $1 - 2\alpha = 0.90$. There would be a 0.90 probability that $\beta_0$ and $\beta_1$ simultaneously fall into the intervals. We would then be 90\% confident that the intervals simultaneously cover $\beta_0$ and $\beta_1$.

On the other hand, if we want to be 95\% confident that the intervals simultaneously cover $\beta_0$ and $\beta_1$, then we need $1 - 2\alpha = 0.95$, which means we need $\alpha = 0.025$. Which means that we need to compute $t(0.025/2; n-2)$. Then we can construct the simultaneous confidence intervals with the t critical value calculated.

In general, when there's $p$ parameters, the probability that they all fall in their respective confidence intervals is $1 - p\alpha$. If we want to be 95\% confident that the intervals simultaneously cover the parameters, then we need that $1 - p\alpha = 0.95$ and $\alpha = 0.05/p$. Then we need to compute $t(0.05/2p; n-p)$, which may not be possible without a computer.

## Example of Simultaneous Confidence Intervals

We examine the rocket propellant data. We do this using R. 

```{r}
rocket = read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Rocket .txt", header=TRUE, sep='\t')
names(rocket)
y = rocket$Shear.strength
x = rocket$Age.of.Propellant
fit = lm(y~x)
confint(fit, level = 1-0.05/2)
```

This way, we have computed simultaneous 95\% confidence intervals for $\beta_0$ and $\beta_1$. The intervals are $[2519.79245, 2735.85227]$ and $[-44.21747, -30.08971]$ for $\beta_0$ and $\beta_1$, respectively.

Alternatively, we can compute the critical value for computing the confidence intervals

```{r}
qt(0.9875, nrow(rocket) - 2)
```

Then we can use this along with the summary data of the model to find the confidence interval.

```{r}
summary(fit)
```

Then the simultaneous confidence intervals are $2627.822 \pm 2.44(44.184)$ for $\beta_0$ and $-37.154 \pm 2.44(2.889)$ for $\beta_1$.

# Model Adequacy Checking

In the previous sections, we talked about simple and multiple linear regression. We recall that the assumptions that we make about the model are

\begin{itemize}
  \item $\epsilon_i$ are normally distributed
  \item $E[\epsilon_i] = 0$ and $Var[\epsilon_i] = \sigma^2$
  \item $\epsilon_i$ are independent
\end{itemize}

We now need to check the assumptions. If the assumptions are not met, then the analysis we do with linear models may not closely reflect the actual model. In this section, we talk about how to check for normality of the error terms and the constancy of variance.

The basic tool that we use to check for model adequacy is analyzing the residuals $e_i = Y_i - \hat{Y_i}$.

## Checking for Normality

To check for normality, we can do this using several ways. We do this by examining the shape of the distribution of the data collected. There are 3 ways in which we can do this:

\begin{itemize}
  \item We can construct boxplots of residuals. Under the normality assumption, the boxplot should show a symmetric box around the median of approximately 0
  \item We can construct a histogram of the residuals. It provides a graphical check on normality because if it shows a bell shape centered at 0 approximately, then we can assume a normal distribution
  \item We can construct a quantile-quantile plot. This plot compares the quantiles of the residual data collected (sample quantiles) with the quantiles from a normal distribution. This is the plot of the ranked residuals against the expected value under normality. We let $e_{(k)}$ be the residual with rank $k$, and $E_k$ be the expected value of the residual with rank $k$ under normality. We have $E_k = \sqrt{MSE} \Phi^{-1}(\frac{k - 0.375}{n + 0.25})$ for $k = 1, ..., n$. We plot $e_{(k)}$ against $E_k$. Under normality, one would expect a straight line pattern in the plot.
\end{itemize}

## Checking for Constancy of Variance

The other assumption we made about our models is that the variance of the random error terms remain constant. We need to use the residuals to verify this assumption.

We note that the variance of the residuals is $Var[e_i] = \sigma^2(1 - h_{ii})$ and the covariance of residuals is $Cov(e_i, e_j) = \sigma^2(1 - h_{ij})$, where $h_{ij}$ is the element of the hat matrix on the ith row and jth column. If we look at the variance of each residual, we notice that the values are different. This means that the residuals have different variances. So, we want to look at the Studentized or standardized residuals $$e_i^* = \frac{e_i}{s\sqrt{1 - h_{ii}}}$$ where $s^2 = MSE$. The semi-studentized residuals are defined as $$\frac{e_i}{\sqrt{1-h_{ii}}}$$ which also has constant variance. 

We can check for constancy of variance by making a plot of the Studentized residuals against the fitted values. If the plot shows a random distribution of the points, then we can assume that the variance of the error terms are constant. However, if we see a telescoping increasing or decreasing pattern among the points, then there is evidence of non-constancy of variance. Then the constancy of variance assumption would be violated.

We can also construct a scale-location plot to examine the homogeneity of the variance of the residuals. This plots the square roots of the absolute values of the Studentized residuals vs. the fitted values.

## Example of checking for model adequacy

We look at the delivery time data. We let $X_1$ represent the delivery number of cases and $X_2$ represent the delivery distance, and we let $Y$ represent the delivery time. 

```{r}
X1 = delivery$Number.of.Cases
X2 = delivery$Distance
Y = delivery$Delivery.Time

fit = lm(Y~X1+X2, data=delivery)
par(mfrow=c(2,2))
plot(fit)
```

From looking at the plots above, we have that the qq-plot follows a straight line pattern in the middle, but not so much on the sides. Therefore, there may be a problem with the normality assumption for this particular dataset. From looking at the residuals vs. fitted plot, we see that the points follow a slight pattern on the plot. This means that the constancy of variance assumption may not be reasonable.

# Remedial Measures and Transformations

There are times when the assumptions for fitting and testing a linear model are violated, but we don't want to immediately discard the model. Instead, we can do some transformations to the response or predictor variables. This may help linearize the model and bring it in line with the assumptions made.

## Variance Stabilizing Transformations

When the assumption that the variance of the error terms is constant is not reasonable, we can then transform the data to make the assumption more reasonable. Below are some examples of transformations we apply to response variables from various distributions.

### Poisson Distribution

In the case that the response variable $Y$ follows a Poisson distribution, then its variance is equal to its mean. If $Y$ is distributed as a Poisson random variable with mean $\lambda$, then $\sqrt{Y}$ is distributed more nearly normally with variance approximately $\frac{1}{4}$ if $\lambda$ is large. In this case, we can regress $\sqrt{Y}$ against $X$ and fit a linear regression model.

### Binomial Distribution

In the case that the response variable $Y$ follows a Binomial distribution $B(n, p)$, we have that its mean is $E[Y] = np$. We use the transformation $$\sin^{-1} \sqrt{\frac{Y+c}{n+2c}}$$ where the optimal value of $c$ is $\frac{3}{8}$ if $E[Y]$ and $n-E[Y]$ are large. The variance is approximately $\frac{1}{4}(n+\frac{1}{2})^{-1}$.

## Transformations to Linearize the Model

Sometimes when we plot $Y$ against $X$, the plot does not look linear. Then we can apply transformations to the response variable $Y$ to make the plot look more linear. Below are some examples:

### Exponential Model

If the true model is $$Y = \beta_0 e^{\beta_1X}\epsilon$$ then we can transform it by taking the logarithms. Here is what the model would look like: $$\ln(Y) = \ln(\beta_0)+\beta_1X+\ln(\epsilon)$$ We still have to make the usual assumptions for a linear model and then verify them.

### Reciprocal Model

The model $Y = \beta_0 + \beta_1X^{-1} + \epsilon$ can be linearized using the trainsformation $X^* = X^{-1}$. This is a transformation on the predictor variable.

The model $\frac{1}{Y} = \beta_0 + \beta_1X + \epsilon$ can be linearized using the reciprocal transformation $Y^* = Y^{-1}$. This is a transformation of the response variable.

The model $Y = \frac{X}{\beta_0 + \beta_1X}$ can be linearized using the reciprocal transformation in 2 steps. First, we use $Y^* = Y^{-1}$ and $X^* = X^{-1}$. Then we have

\begin{equation*}
\begin{split}
  Y
  &= \frac{X}{\beta_0 + \beta_1X} \\
  &= \frac{1}{\beta_0X^{-1} + \beta_1} 
\end{split} 
\end{equation*}

Consequently, we get that $Y^{-1} = \beta_0X^{-1} + \beta_1$. Then we obtain $Y^* = \beta_1 + \beta_0X^*$.

## Box-Cox Transformations

The data may not appear to be normally distributed sometimes. Then Box and Cox suggested a power transformation of the type

$$
  Y^{(\lambda )} = \left \{
  \begin{array}{ll}
    \frac{Y^{\lambda} - 1}{\lambda \dot{Y}^{\lambda - 1}} \, \quad \lambda \neq 0 \\
    \dot{Y}\ln(Y)  \quad \lambda = 0 \,
  \end{array}
  \right.
$$
where $$\dot{Y} = \ln^{-1}[\frac{\sum \ln(Y_i)}{n}]$$

The value of $\lambda$ can be estimated using trial and error. We fit a model for $Y^{(\lambda)}$ for various values of $\lambda$ and selecting the one which mimimizes the error sum of squares from a graphical plot. We can also construct a confidence interval for $\lambda$. Using R, we can estimate $\lambda$ using maximum likelihood.

The theory behind the transformation is as follows:

The original model was assumed to be $\boldsymbol{Y}\sim N_n(\boldsymbol{X\beta}, \sigma^2\boldsymbol{I})$

Then the transformed model is $Y^{(\lambda)}$ and has likelihood function given by $$\frac{1}{(2\pi)^{n/2}\sigma^n} exp(-\frac{(\boldsymbol{y}^{(\lambda)} - \boldsymbol{X\beta})^T(\boldsymbol{y}^{(\lambda)} - \boldsymbol{X\beta})}{2\sigma^2}) J(\lambda; \boldsymbol{y})$$ with parameters $\boldsymbol{\beta}$ and the Jacobian for the transformation $$J(\lambda; \boldsymbol{y}) = \prod_{i=1}^{n} |\frac{\partial y_i^{(\lambda)}}{\partial y_i}|$$
The maximum likelihood estimator of the variance is given by $$\hat{\sigma^2} = \frac{\boldsymbol{(Y^{(\lambda)})^T}[\boldsymbol{I - X(X^TX)^{-1}X^T}] \boldsymbol{Y^{(\lambda)}}}{n}$$

The maximum log likelihood for fixed $\lambda$ is $$L_{max} = -\frac{n}{2}\log(\hat{\sigma^2}) + \log(J(\lambda; \boldsymbol{y})) = -\frac{n}{2}\log(\hat{\sigma^2}) + (\lambda - 1)\sum \log(y_i)$$

We then plot $L_{max}(\lambda)$ against $\lambda$ to find the value of $\lambda$ which yields the maximum.

## Weighted Least Squares

This approach is for when the constancy of variance assumption is not reasonable. We assume that $Var[e_i] = \sigma_i^2$. Instead of minimizing the sum of the squared errors, we can minimize the sum of the weighted squared errors $$\sum w_ie_i^2$$ where the weights satisfy $$Var[\sqrt{w_i}e_i] = \sigma^2$$. In other words, we're to minimize $$\sum w_i[Y_i - \beta_0 - \beta_1X_i]$$ with respect to $\beta_0$ and $\beta_1$.

### Estimation and Fitting

With ordinary least squares regression, we have that the linear regression model is $$\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon}$$

We define the matrix $\boldsymbol{W}$ as the matrix of weights $$\boldsymbol{W} = \begin{pmatrix} w_1 & 0 & \dots & 0 \\ 0 & w_2 & \dots & 0 \\ \dots & \dots & \dots & \dots \\ 0 & 0 & \dots & w_n \end{pmatrix}$$

Then the original model goes to $$\boldsymbol{W^{1/2}Y} = \boldsymbol{W^{1/2}X\beta} + \boldsymbol{W^{1/2}\epsilon}$$, where $$\boldsymbol{W}^{1/2} = \begin{pmatrix} w_1^{1/2} & 0 & \dots & 0 \\ 0 & w_2^{1/2} & \dots & 0 \\ \dots & \dots & \dots & \dots \\ 0 & 0 & \dots & w_n^{1/2} \end{pmatrix}$$

Then we get that the least squares estimate for $\boldsymbol{\beta}$ is 

\begin{equation*}
\begin{split}
  \boldsymbol{b_W} 
  &= \boldsymbol{((W^{1/2}X)^T(W^{1/2}X))^{-1}(W^{1/2}X)(W^{1/2}Y)} \\
  &= \boldsymbol{(X^TW^{1/2}W^{1/2}X)^{-1}X^TW^{1/2}W^{1/2}Y} \\
  &= \boldsymbol{(X^TWX)^{-1}X^TWY}
\end{split}
\end{equation*}

The MSE becomes

\begin{equation*}
\begin{split}
  MSE_{W}
  &= \frac{\sum w_i(Y_i - \hat{Y_i})^2}{n - p} \\
  &= \frac{\sum w_i(e_i)^2}{n - p}
\end{split}
\end{equation*}

### Choosing weights

We want to now choose the appropriate weights for weighted least squares. There are many ways we can do this. For example, we can take $w_i = \sqrt{X_i}$ or $w_i = \sqrt{Y_i}$. However, there is one way that we can commonly use. We first divide up the data into clusters using the $X$ variable. Then, we estimate the sample variances of the $Y_i$ for each group, denote them as $s_i^2$. Then we compute the averages of the $X_i$ values in each cluster. We fit a regression model of the variances of $Y_i$ against the averages of $X_i$ in each cluster. We then estimate the variances by substituting all the $X_i$ values into the equation of the regression function of the variances against the averages. The weights are the inverses of the estimated variances.

For example, we use the turkey data. We group the data into clusters, then we obtain the averages of the $X_i$ in each group and the variances of $Y_i$ in each group. We obtain the regression function $s^2 = 1.5329 - 0.7334\overline{X} + 0.0883\overline{X}^2$. Then we substitute the individual $X_i$ in the data to find $\hat{s_i^2} = 1.5329 - 0.7334X_i + 0.0883X_i^2$. Then we obtain the weights as $\frac{1}{s_i^2}$. With the usual linear regression model, we get that the regression function is $$Y = -0.579 + 1.14X$$ and the weighted regression is $$Y = -0.892 + 1.16X$$

## Breusch-Pagan Test

The Breusch-Pagan Test is used for checking for constancy of variance. It assumes the model $$\log(\sigma_i^2) = \gamma_0 + \gamma_1X_i$$. We want to test the hypothesis $H_0: \gamma_1 = 0$. To do this, we regress the squared residuals $e_i^2$ against the predictors $X_i$. Then we can calculate the regression sums of squares of the model $SSR^*$. Let $SSE$ represent the error sum of squares when we fit a model for $Y$ against $X$. The test statistic is then $$\chi^2_{BP} = \frac{\frac{SSR^*}{2}}{\frac{SSE}{n}} \sim \chi^2(1)$$ We reject $H_0$ for large values of the test statistic.

We can also use R to do the test. For the purpose, we use the Turkey data. Then we get that 

```{r, include=FALSE}
library(lmtest)
```

```{r}
turkey = read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Turkey data.txt", header=TRUE , sep='\t')
names(turkey)
y <- turkey$Weight
x <- turkey$Age
fit <- lm(y~x)
bptest(fit)
```
We obtain a test statistic of 2.5466 and a p-value of 0.11. This means that we fail to reject $H_0$. We can assume that the variance of error terms are constant.

## Example: Electricity Data

In this example, we look at model adequacy checking and the Box-Cox transformation

We want to analyze the electricity data. We first load the data

```{r}
electric = read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Electric Utility Data.txt", header=TRUE, sep='\t')
names(electric)
```

Now, we fit a linear regression model and look at the summary and ANOVA table

```{r}
y = electric$Demand.Y
x = electric$Usage.X

model = lm(y~x, data=electric)
anova(model)
summary(model)
```

From looking at the summary table, we find that a simple linear regression model for this data is $\hat{Y} = -0.6427556 + 0.0035502X$, where $Y$ is the electric demand and $X$ is the electric usage. From looking at both the t statistics and the ANOVA table, we can reject the null hypothesis $H_0: \beta_1 = 0$. There is evidence of relationship between electric usage and demand.

Now, we want to actually verify that the model we fitted actually make sense. We can now do the model adequacy checking. We first create a histogram of the residuals and a normal curve.

```{r, out.width="50%"}
library(MASS)
sresid = studres(model)
hist(sresid, freq=FALSE)
smodel=seq(min(sresid),max(sresid),length=25)
ymodel=dnorm(smodel)
lines(smodel, ymodel)
curve(dnorm(x, mean(x), sd(x)))
```
If we look at the histogram of the residuals and the normal curve, we see that the residuals are approximately normally distributed with mean 0. This means that we can assume that the random error terms follow a normal distribution.

Usually the Box-Cox transformation is done on data that doesn't seem normal, but we can still try to find a value for $\lambda$ for Box-Cox transformation.

```{r}
b=boxcox(model)
b
```
From the graph above, we find that the optimal value for $\lambda$ is between 0.5 and 1. We can find the actual value using the following command:

```{r}
b$x[which.max(b$y)]
```

Therefore, the optimal value for $\lambda$ for the Box-Cox transformation is 0.5454545.

Now, we create diagnostic plots for the data

```{r}
par(mfrow=c(2,2))
plot(model)
```

From looking at the normal probability plot (qq plot), we find that the normality assumption is reasonable. However, from looking at the scale-location plot, there seems to be a pattern among the points. This means that there may be a problem with the constancy of variance assumption. 

We try a transformation by taking the square root of the response variable and regressing it against the predictor.

```{r}
model1 = lm(sqrt(y)~x, data=electric)
summary(model1)
anova(model1)

```

From the summary and ANOVA table above, we find that the regression model is $\sqrt{\hat{Y}} = 0.06212411 + 0.0009231X$. The t statistics and the ANOVA table all suggest that we can reject $H_0: \beta_1 = 0$. This means that there is significant evidence that there is a relationship between $\sqrt{Y}$ and $X$ and hence, there is a relationship between $Y$ and $X$.

We now create the diagnostic plots and see if there is an improvement.

```{r}
par(mfrow=c(2,2))
plot(model1)
```

We see that the residual plot and the scale-location plots now show a more random pattern and the normal qq plot still follows a straight-line pattern. Therefore, the transformation does improve the model.

Now, we want to show the Breusch-Pagan test. We do this using the following command:

```{r}
library(lmtest)
bptest(model)
bptest(model1)
```

From looking at the Breusch-Pagan results, the original model has a p-value of 0.04261, which means that we reject the null hypothesis that there is no relationship between the variance of the error terms and the predictor. Therefore, there is convincing evidence to suggest that the variance of the error terms is not constant. The transformed model has a p-value of 0.3512, which means that we fail to reject the null hypothesis. Therefore, we can assume that the random error terms have constant variance.

# Regression Diagnostics for Leverage and Measures of Influence

## Leverages

There are times when a single observation may influence the results of a regression analysis. Hence, the detection of such influential observations is important. For this purpose, the hat matrix plays a very important role.

We begin with the minimized sum of squares:

\begin{equation*}
\begin{split}
  R(\boldsymbol{b})
  &= (\boldsymbol{Y - Xb})^T(\boldsymbol{Y - Xb}) \\
  &= \boldsymbol{Y^TY - Y^TX(X^TX)^{-1}X^TY} \\
  &= \boldsymbol{Y^T(I - X(X^TX)^{-1}X^T)Y} \\
  &= \boldsymbol{Y^T(I-H)Y}
\end{split}
\end{equation*}

where $\boldsymbol{H} = \boldsymbol{X(X^TX)^{-1}X^T}$ is the hat matrix.

If we let $\boldsymbol{x_i}^T$ be the i-th row of $X$. Then the i-th diagonal of $\boldsymbol{H}$ is for $i = 1, ..., n$ $$h_{ii} = \boldsymbol{x_i}^T(\boldsymbol{X^TX})^{-1}\boldsymbol{x_i}$$

In the case of simple linear regression with $p = 2$, we have $\boldsymbol{x_i}^T = [1, X_i]$. Then we get

\begin{equation*}
  \begin{split}
    h_{ii}
    &= \frac{\boldsymbol{x_i}^T \begin{pmatrix} \sum X_j^2 & -\sum X_j \\ -\sum X_j & n \end{pmatrix} \boldsymbol{x_i}}{n\sum X_j^2 - (\sum X_j)^2} \\
    &= \frac{\sum X_j^2 - 2X_i\sum X_j + nX_i^2}{n \sum X_j^2 - (\sum X_j)^2} \\
    &= \frac{\sum X_j^2 - n\overline{X}^2 + n\overline{X}^2 - 2nX_i\overline{X} + nX_i^2}{n\sum X_j^2 - (\sum X_j)^2} \\
    &= \frac{\sum(X_j - \overline{X})^2 + n\overline{X}^2 + 2nX_i\overline{X} + nX_i^2}{n \sum (X_j - \overline{X})^2} \\
    &= \frac{\sum (X_j - \overline{X})^2 + n(X_i - \overline{X})^2}{n \sum (X_j - \overline{X})^2} \\
    &= \frac{1}{n} + \frac{(X_i - \overline{X})^2}{\sum (X_j - \overline{X})^2}
  \end{split}
\end{equation*}

The quantity $h_{ii}$ is called the leverage of the i-th observation.

A further insight is gained by writing the mean $\overline{X}$ in terms of the mean $\overline{X}_{(i)}$ when the i-th observation is deleted. We can show $$\overline{X} = \frac{\sum X_j}{n} = \frac{X_i + \sum_{j \neq i} X_j}{n} = \frac{X_i + (n-1)\overline{X}_{(i)}}{n}$$ so that $$X_i - \overline{X} = X_i - \frac{1}{n}(X_i + (n-1) \overline{X}_{(i)}) = \frac{n-1}{n}(X_i - \overline{X}_{(i)})$$

Hence, we get $$h_{ii} = \frac{1}{n} + \frac{(X_i - \overline{X})^2}{\sum (X_j - \overline{X})^2} = \frac{1}{n} + (\frac{n-1}{n})^2 \frac{(X_i - \overline{X}_{(i)})^2}{\sum (X_j - \overline{X})^2}$$ This shows that the leverage of the i-th observation will be large if $X_i$ is far from the mean of the other observations.

The leverage can be used to flag influential observations. This follows from the fact that 

\begin{equation*}
  \begin{split}
  Trace[\boldsymbol{H}]
  &= Tr[\boldsymbol{X(X^TX)^{-1}X^T}] \\
  &= Tr[\boldsymbol{(X^TX)^{-1}}X^TX] \\
  &= Tr[\boldsymbol{I_p}] \\
  &= p
  \end{split}
\end{equation*}

Hence, the average $\frac{\sum h_ii}{n} = \frac{p}{n}$

Consequently, observations with a value of the leverage greater than twice the average should be flagged. (i.e. $h_{ii} > 2(\frac{p}{n})$).

## DFFITS

A useful measure of the influence that case $i$ has on the fitted value $\hat{Y}$ is given by $$DFFITS_i = \frac{\hat{Y_i} - \hat{Y_i}_{(i)}}{\sqrt{MSE_{(i)} h_{ii}}} = t_i(\frac{h_{ii}}{1 - h_{ii}})^{1/2}$$ where $$t_i = e_i(\frac{n-p-1}{SSE(1-h_{ii}) - e_i^2})^{1/2}$$ where we have $\hat{Y_i}$ is the fitted value of $Y$ at $X_i$, $\hat{Y}_{(i)}$ 0s the fitted value at $X_i$ with the i-th observation removed, $MSE_{(i)}$ is the MSE calculated with teh i-th case removed, ahd $h_{ii}$ is the i-th diagonal of the hat matrix.

This shows that it can be calculated from the residuals, the error sum of squares, and the hat matrix values. The value of $DFFITS_i$ represents the number of estimated standard deviations of $\hat{Y_i}$ that the fitted value increases or decreases with the inclusion of the i-th case in fitting the regression model. If $DFFITS_i$ is high, then it is influential on the fitted value.

If case $i$ is an $X$ outlier and has high leverage, then $(\frac{h_{ii}}{1-h_{ii}})^{1/2} > 1$ and $DFFITS$ will be large in absolute value. As a guideline, influential cases are flagged if $$|DFFITS_i| > 1$$ for small to medium data sets and $$|DFFITS_i| > 2\sqrt{\frac{p}{n}}$$ for large data sets. This is a rule of thumb we use to flag influential values using DFFITS.

## Cook's Distance

The next metric we'll be talking about is the Cook's distance. The difference between Cook's distance and DFFITS is that Cook's distance considers the influence of the i-th case on the entire collection of $n$ fitted values instead of just one fitted value. The formula for cook's distance is $$D_i = \frac{\sum_{j=1}^{n} (\hat{Y_i} - \hat{Y_j}_{(i)})^2}{pMSE} = \frac{e_i^2}{pMSE}(\frac{h_{ii}}{(1-h_{ii})^2})$$ Cook's distance is a function of the residual $e_i$ and the leverage $h_{ii}$. It can be large if either the residual is large and the leverage moderate, or if the residual is moderate and the leverage is large, or both are large. It can be shown that $D_i \sim F(p, n-p)$. Since $F_{0.50}(p, n-p) \approx 1$, we consider points for which $D_i > 1$ to be influential. Ideally, we want the estimated $\hat{\beta}_{(i)}$ to be within the boundary of the 10 - 20\% confidence region. In R these regions are indicated in red.

## DFBETAS

DFBETAS are a measure for the influence that case $i$ has on each of the regression coefficients $b_k$, $k = 0, 1, ..., p-1$. $$DFBETAS_{(i)} = \frac{b_k - b_{k(i)}}{\sqrt{MSE_{(i)}c_{ii}}}$$ where $c_{ii}$ is the i-th diagonal element of $(\boldsymbol{X^TX})^{-1}$.

A large value of $DFBETAS_{(i)}$ indicates a large impact of the i-th case on the k-th regression coefficient. As a guideline, we flag the observation as influential if $DFBETAS_{(i)} > \frac{2}{\sqrt{n}}$ for large data sets and $DFBETAS_{(i)} > 1$ for small datasets.

## Example: Body Fat Data Analysis

We now look at the Bank data set. We first load the data:

```{r}
bank = read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Bank Data.txt", header=TRUE, sep="\t")
names(bank)
```

Now, we look at the diagnostic measures on each observation and determine which ones are influential.

```{r, include=FALSE}
library(olsrr)
```

```{r}
y <- bank$Number.New.accounts
x <- bank$Minimum.Deposit

fit <- lm(y~x)

# Cook's Distance vs. Observations
ols_plot_cooksd_bar(fit)
```

From looking at the cook's distance chart, we find that the 4th observation is influential on the fitted values of the model at all $X$ values.

Now, we construct a plot for DFFITS

```{r}
ols_plot_dffits(fit)
```

From looking at the plot of DFFITS vs. Observations, we find that the 4th observation is influential on the 4th fitted value of the model.

Now, we construct plots of DFBETAS vs. Observations for each regression coefficient:

```{r}
ols_plot_dfbetas(fit)
```

From the DFBETAS vs. Observations plots, we find that the 4th and 7th observations are influential on the intercept, and the 4th, 7th, and 10th observations are influential on the slope.

We can also look at the diagnostic metrics themselves using the following command:

```{r}
influence.measures(fit)
```

From looking at the above table, it indicates that the 4th observation is the most influential on the regression model.

# Different Regression Models

In this section, we talk about different regression models other than the linear regression models discussed in previous sections. We talk about 2 specific models in this section: Polynomial and Indicator Variables

## Polynomial Regression

Sometimes a linear model may not be the appropriate fit for a dataset. A polynomial regression can sometimes be a better fit.

A k-order polynomial regression model in one variable takes the form $$Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \dots + \beta_kX^k + \epsilon$$ which may be fitted using the matrix approach. It is important to keep in mind that in considering such a model, the order k should be as low as possible. If the value of $k$ gets too high, then the inversion of the matrix $X^TX$ may be inaccurate or not even exist, which would result in poor estimates of the parameters and their variances.

Often, orthogonal polynomials are used in the modeling because they simplify the fitting process: $$Y_i = \beta_0P_0(X_i) + \beta_1P_1(X_i) + \beta_2P_2(X_i) + \dots + \beta_kP_k(X_i) + \epsilon_i$$ where $P_j$ is a $j$ order orthogonal polynomial satisfying $$\sum_{i=1}^{n} P_j(X_i)P_l(X_i) = 0$$ for all $j \neq l$ and we have $P_0(X_i) = 1$.

Such polynomials have been tabulated. The least squares estimates are given by $$\hat{\beta_j} = \frac{\sum_{i=1}^{n} P_j(X_i)Y_i}{P_j^2(X_i)}$$ for $j = 0, 1, ..., k$.

The principal advantage of using orthogonal polynomials is that the model can be fitted sequentially. This specific advantange is less important today in the age of high speed computing compared to the times when much of the modeling was done using calculators.

When two or more variables are involved, interaction terms are included as in the following model involving two variables: $$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_{11}X_1^2 + \beta_{22}X_2^2 + \beta_{12}X_1X_2 + \epsilon$$

Such models are called response surfaces. They are often used in control theory problems to optimize the selection of control settings of the variables.

## Indicator Regression Models

Regression analysis allows the use of indicator variables which are qualitative or categorical in nature. Such variables are labeled dummy variables. For example, to take into account gender we may define 

$$
  X_2 = \left \{
  \begin{array}{ll}
    1  \quad \text{male} \\
    0  \quad \text{female} \,
  \end{array}
  \right.
$$
An interesting application is to the case where one wishes to fit a simple linear model as a function of gender. Set $$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \epsilon$$

In that case, we have

$$
  Y = \left \{
  \begin{array}{ll}
    \beta_0 + \beta_1X_1 + \beta_2 + \epsilon \quad \text{male} \\
    \beta_0 + \beta_1X_1 + \epsilon \quad \text{female} \,
  \end{array}
  \right.
$$
So here the lines are parallel.

This can also be generalized to 2 or more dummy variables.

## Example: Turkey Data Analysis

We now consider the turkey data. We first load the data into a dataframe.

```{r}
Turkey <- read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Turkey data.txt", header=TRUE, sep="\t")
Turkey
```

We want to first fit a regression model without considering the origin of the data. We only consider the turkey weights and ages.

```{r}
Age <- Turkey$Age
Weight <- Turkey$Weight
Origin <- Turkey$Origin
Z1 <- Turkey$Z1
Z2 <- Turkey$Z2
plot(Age, Weight)
model = lm(Weight~Age)
anova(model)
summary(model)
```

From looking at the summary above, we find that there is a significant relationship between Turkey age and weight as shown by the F statistic in the ANOVA table and the t statistic in the summary table. They both yield low p-values, which means we can reject $H_0: \beta_1 = 0$. We also find that the $R^2$ value is 0.6647, which means that around 66.47\% of variability is explained by the model.

Now, we create diagnostic plots to check for model adequacy:

```{r}
Residuals <- model$residuals
plot(model,1)
plot(model,2)
plot(model,3)
plot(model,4)
plot(model,5)
plot(model,6)
```
From looking at the normal qq-plot, we find that theere is some variability among the points, so the normality assumption may not be reasonable in this model. From looking at the residuals vs. fitted plot, there seems to be a curve pattern among the points so a linear model may not be the best fit. We also find that the 2nd observation is influential on the fitted values from looking at the Cook's distance plot.

Now, we fit a model with weight, age, and origin. To add origin as a predictor, we add the dummy variables $Z_1$ and $Z_2$ into the model.

```{r}
model1 <- lm(Weight~Age+Z1+Z2, data=Turkey)
anova(model1)
summary(model1)
```

From looking at both the ANOVA table and the summary table, we find that there is a significant relationship between Turkey weight and age, $Z_1$, and $Z_2$, which means that there is a significant relationship between weight and age and between weight and origin. The $R^2$ value is 0.9794, which is a significant improvement compared to the model without origin as a variable. This means that 97.94\% of variability is explained by the model.

Now, we create plots to check for model adequacy

```{r}
Residuals1 <- model1$residuals
plot(model1,1)
plot(model1,2)
plot(model1,3)
plot(model1,4)
plot(model1,5)
plot(model1,6)
plot(Weight,Residuals)
plot(Weight,Residuals1)
```

The normal qq-plot is significantly improved compared to the previous model. The points follow a straight-line pattern closely. This means that the normality assumption is reasonable. The residuals vs. fitted plot indicates that a linear model may not be the best fit since there appears to be a slight curve pattern. From the Cook's distance plot, we find that the second observation is influential on the fitted values.

## Example: Hardwood Data

Now, we analyze the Hardwood data. We look at the polynomial regression model in this case. We first load the data.

```{r}
Hardwood <- read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Hardwood Data.txt",
                       header=TRUE, sep="\t")
Hardwood
```

We first fit a linear model of tensile strength vs. concentration.

```{r}
x <- Hardwood$Concentration
y <- Hardwood$Tensile.Strength.Y
x2 = x*x
plot(Hardwood)
```

From looking at the plot, it seems like the points do not follow a linear pattern, so a linear model likely won't be a good fit for the data. It looks more like a quadratic pattern. We first try fitting a linear model, then we try fitting a quadratic model.

```{r}
model <- lm(y~x, data=Hardwood)
anova(model)
summary(model)
```

From looking at the ANOVA table and the summary of the model, we find that there is a significant relationship between tensile strength and concentration. However, the relationship is only significant at the 0.05 level. The $R^2$ value is 0.3054, which means that only 30.54\% of variability is explained by the model, which is low. 

Now, we fit a polynomial regression model and see if we can improve the relationship:

```{r}
model2 <- lm(y~x+x2)
anova(model2)
summary(model2)
```
The quadratic model is a much better fit. There seems to be a highly significant relationship between tensile strength and concentration. The $R^2$ value is 0.9085, which means that around 90.85\% of variability in $Y$ is explained by the model. This is significantly higher than the original linear model without the quadratic term.

Now, we check for model adequacy:

```{r}
par(mfrow = c(2,2))
plot(model2)
```

There is evidence of a nonlinear relationship between $X$ and $Y$ since there is a curve pattern in the residuals vs. fitted plot. It also means that the constancy of variance assumption is not always reasonable. We also have that the normal qq-plot doesn't follow a straight line pattern, so the normality assumption also doesn't seem reasonable with the model.

# Multicollinearity and Ridge Regression

When there is correlation among the predictors themselves in a linear regression model, then we say that multicollinearity exists. In this section, we talk about some potential signs of multicollinearity, diagnostics for multicollinearity, and remedial measures to overcome this problem.

## Multicollinearity overview

Some symptoms of multicollinearity include large variation in the estimated coefficients when a new variable is either added or deleted, when there are non significant results in individual tests on the coefficients of important variables, when there's large coefficients of simple correlation between pairs of variables, and when the confidence intervals for the regression coefficients of important variables are too wide.

When the variables are correlated among themselves, we say that multicollinearity exists. The principal difficulty is that the matrix $\boldsymbol{X^TX}$ may not be invertible. Multicollinearity also affects the interpretation of the coefficients as they may vary in value. Consider the case of two predictor variables $X_1$, $X_2$. If the variables are standardized, then we have the matrix $$\boldsymbol{X^TX} = \begin{pmatrix} 1 & r_{12} \\ r_{12} & 1 \end{pmatrix}$$ where $r_{12}$ is the correlation between the two variables. Then we get that $$\boldsymbol{(X^TX)^{-1}} = \frac{1}{1 - r_{12}^2} \begin{pmatrix} 1 & r_{12} \\ r_{12} & 1 \end{pmatrix} $$ and consequently, the variance-covariance matrix of the coefficients is $$\sigma^2(X^TX)^{-1} = \sigma^2 \frac{1}{1 - r_{12}^2} \begin{pmatrix} 1 & -r_{12} \\ -r_{12} & 1 \end{pmatrix}$$ As a result, as $|r_{12}| \rightarrow 1$, we have that the variance of the individual coefficients approach $\infty$. As $r_{12} \rightarrow \pm 1$, we have that the covariance of the coefficients approach $\pm \infty$. This means that the variance and covariance of the estimates of the coefficients grows larger when the correlation between individual coefficients gets closer to 1. In other words, multicollinearity causes the variance of the coefficients to become larger.

In general, the diagonal elements of $\boldsymbol{(X^TX)^{-1}}$ are $C_{jj} = \frac{1}{1 - R_j^2}$ where $R^2_j$ is the R-squared value obtained from the regression of $X_j$ on the other $p - 1$ variables. If there is strong multicollinearity between $X_j$ and the other $p - 1$ variables, then $R^2_j \approx 1$ and $Var[\hat{\beta_j}] \approx \infty$.

Under multicollinearity, the values of the estimates will also be large. We set $$L = ||\boldsymbol{\hat{\beta} - \beta}||^2$$ Then, 

\begin{equation*}
  \begin{split}
    E[\sum_{j = 1}^{p} (\hat{\beta_j} - \beta_j)^2]
    &= \sum_{j = 1}^{p} Var[\hat{\beta_j}] \\
    &= \sigma^2 Trace[\boldsymbol{(X^TX)^{-1}}] \\
    &= \sigma^2 \sum_{j=1}^{p} \frac{1}{\lambda_j}
  \end{split}
\end{equation*}

where $\{\lambda_j\}$ are the eigenvalues of $\boldsymbol{X^TX}$.

Under multicollinearity, some of these eigenvalues will be small and hence their inverses will be large. 

We expand the loss function $L$, then we get $L = \boldsymbol{\hat{\beta}^T\hat{\beta} - 2\hat{\beta}\beta + \beta^T\beta}$. Taking the expectation, we get

\begin{equation*}
  \begin{split}
    E[L]
    &= E[||\boldsymbol{\hat{\beta}}||^2] - ||\boldsymbol{\beta}||^2 \\
    &= \sigma^2 \sum_{j=1}^{p} \frac{1}{\lambda_j}
  \end{split}
\end{equation*}

Hence, $E[||\boldsymbol{\hat{\beta}}||^2] = ||\boldsymbol{\beta}||^2 + \sigma^2 \sum_{j=1}^{p} \frac{1}{\lambda_j}$. This shows that the estimators are large when the inverses of the eigenvalues are large as a result of multicollinearity. In conclusion, multicollinearity causes the estimates of coefficients and the variances of the coefficient estimates to be large.

## Multicollinearity Diagnostics

We start by standardizing the variables.

$$Y_i^* = \frac{1}{\sqrt{n-1}}(\frac{Y_i - \overline{Y}}{s_Y})$$
$$X_{ik}^* = \frac{1}{\sqrt{n-1}}(\frac{X_{ik} - \overline{X_k}}{s_k})$$
where $s_Y$ is the standard deviation of the observations of $Y_i$ and $s_k$ is the standard deviation of the observations of $X_{ik}$.

Then we get that $$Y_i^* = \sum_{k=1}^{p-1} \beta_i^*X_{ik}^* + \epsilon_i^*$$

$$\beta_k = (\frac{s_Y}{s_k})\beta_k^*$$
$$\beta_0 = \overline{Y} - \sum_{i=1}^{p-1} \beta_i\overline{X}_i$$
$$r_{XX}b^* = r_{YX}$$
where $r_{XX}$ is the correlation matrix $$r_{XX} = \begin{pmatrix} 1 & r_{12} & \dots & r_{1, p-1} \\ r_{12} & 1 & \dots & r_{2, p-1} \\ \dots & \dots & \dots & \dots \\ r_{1, p-1} & \dots & \dots & 1 \end{pmatrix}$$ and $$r_{YX} = \begin{pmatrix} r_{Y1} \\ r_{Y2} \\ \dots \\ r_{Y, p-1} \end{pmatrix}$$ We also have $r_{ij} = Corr(X_i, X_j)$ and $r_{Yi} = Corr(Y, X_i)$.

We now talk about diagnostics for multicollinearity. We first talk about the variance inflation factors (VIFs). Suppose that the regression is fitted using standardized predictor variables. Then we have $Var[\boldsymbol{b}] = \sigma^2 r_{XX}^{-1}$, where $r_{XX}$ is the matrix of pairwise correlation coefficients among the predictors. We define the variance inflation factor (VIF) $$(VIF)_k = (1 - R^2_{k})^{-1}$$ where $R^2_k$ is the coefficient of multiple determination where $X_k$ is regressed on the $p-2$ other $X$ variables. Hence, $Var[b_k] = \sigma^2(1 - R^2_k)^{-1}$. This means that the VIFs can inflate the variances of the estimates of the coefficients, which is a sign of multicollinearity.

For each variable $X_k$, we have that $(VIF)_k = 1$ when $R^2_k = 0$. Whenever $X_k$ is not linearly related to the other $X$ variables in the model. Under perfect correlation ($R^2_k = 1$), the variance is unbounded. As a rule of thumb, a value $(VIF)_k > 10$ indicates that multicollinearity exists. Alternatively, we can compute the average $$\overline{VIF} = \frac{\sum (VIF)_k}{p-1}$$ Mean values much greater than 1 point to serious multicollinearity.

We now talk about some other diagnostic measures for multicollinearity. As mentioned previously, the eigenvalues $\lambda_i$ of the matrix $\boldsymbol{X^TX}$ can be used to measure the extend of multicollinearity in the system. If one or more are small, then there are near linear dependencies in the columns of $\boldsymbol{X^TX}$.

The condition number $\kappa$ and condition indices $\kappa_j$ of $\boldsymbol{X^TX}$ are defined to be $$\kappa = \frac{\lambda_{max}}{\lambda_{min}}$$ and $$\kappa_j = \frac{\lambda_{max}}{\lambda_j}$$

We use the following rule of thumb: we say that there is no serious multicollinearity if $\kappa < 100$, there is moderate to strong multicollinearity if $100 < \kappa < 1000$, and there is severe multicollinearity if $\kappa > 1000$.

## Ridge Regression

Ridge regression is considered as a remedial measure to multicollinearity. The theory is as follows. We first transform the normal equation using standardized variables, so it goes from $(X^TX)b = X^TY$ to $r_{XX}b = r_{YX}$.

Instead of solving the original normal equation, we instead solve the equation $$(r_{XX} + cI)b^R = r_{YX}$$ where $c > 0$ is a constant and $b^R$ is the ridge regression estimate of the parameters. The standardized ridge regression coefficients become $$b^R = (r_{XX} + cI)^{-1}r_{YX}$$

The constant $c$ reflects the fact that the ridge estimators will be biased (i.e. $E[b^R] \neq \beta^*$) but they tend to be more stable or less variable than the ordinary least squares estimators.

The constant $c$ is usually chosen in such a way that the estimators of $b_k^R$ are stable in value. Alternatively, whenever the $(VIK)_k$ are stable in value. A plot of coefficients against $c$ is called the ridge trace and this helps in the selection of $c$.

Ridge regression can also be obtained from the method of penalized regression. We have the following system of equations:

$$
\begin{aligned}
  (1+c)b_1^R + r_{12}b_2^R + \dots + r_{1, p-1}b_{p-1}^R = r_{Y1} \\
  r_{21}b_1^R + (1 + c)b_2^R + \dots + r_{2, p-1}b_{p-1}^R = r_{Y2} \\
  \dots \\
  r_{p-1,1}b_1^R + r_{p-1,2}b_2^R + \dots + (1+c)b_{p-1}^R = r_{Y, p-1}
\end{aligned}
$$
This is also the same as solving the penalized least squares $$Q = \sum [Y_i - \beta_1X_{i1} - \dots - \beta_{p-1}X_{i, p-1}]^2 + c\sum_{j=1}^{p-1} \beta_j^2$$

If we differentiate the equation above with respect to each of the parameters, we get the system of equations above. In the above equation, the penalty function is $c\sum_{j=1}^{p-1} \beta_j^2$

The major drawback of ridge regression is that the ordinary inference procedures are no longer applicable (i.e. ANOVA, t-tests, correlation tests). We need to use resampling methods to obtain the precision of the estimators.

Another approach is to use a penalty function $$c \sum_{j=1}^{p-1} |\beta_j|$$ which permits some regression coefficients to be 0. This is know as LASSO regression, which stands for Least Absolute Shrinkage and Selection Operator.

## Example: Body Fat Data Analysis

We now look at the Body Fat Data. We first load the data into a dataframe:

```{r}
Tricepts <- c(19.5, 24.7, 30.7, 29.8, 19.1, 25.6, 31.4, 27.9, 22.1, 25.5, 31.1, 30.4, 18.7, 19.7, 14.6, 29.5, 27.7, 30.2, 22.7, 25.2)
Thigh <- c(43.1, 49.8, 51.9, 54.3, 42.2, 53.9, 58.5, 52.1, 49.9, 53.5, 56.6, 56.7, 46.5, 44.2, 42.7, 54.4, 55.3, 58.6, 48.2, 51.0)
Midarm <- c(29.1, 28.2, 37.0, 31.1, 30.9, 23.7, 27.6, 30.6, 23.2, 24.8, 30.0, 28.3, 23.0, 28.6, 21.3, 30.1, 25.7, 24.6, 27.1, 27.5)
Fat <- c(11.9, 22.8, 18.7, 20.1, 12.9, 21.7, 27.1, 25.4, 21.3, 19.3, 25.4, 27.2, 11.7, 17.8, 12.8, 23.9, 22.6, 25.4, 14.8, 21.1)

Bodyfat <- data.frame(Tricepts, Thigh, Midarm, Fat)
Bodyfat
```

Now, we construct a scatterplot matrix and a correlation matrix to detect possible multicollinearity problems:

```{r}
plot(Bodyfat)
cor(Bodyfat)
```
From the scatterplot matrix, we see that there is positive correlation between Fat and Tricepts and between fat and Thigh, but there does not seem to be a strong correlation between Fat and Midarm. We also see that Tricepts and Thigh also appear to be highly positively correlated.

From the correlation matrix, we find that there is strong and positive correlation between Fat and Tricepts and between Fat and Thigh, but a very weak correlation between Fat and Midarm. There is also very strong correlation between Tricepts and Thigh.

The above observations show that there are potential multicollinearity problems in the dataset. 

We fit a multiple linear regression model of Fat vs. Tricepts, Thigh, and Midarm:

```{r}
model <- lm(Fat ~ Tricepts + Thigh + Midarm, data=Bodyfat)
summary(model)
```
We find that none of the coefficients, including the intercept, are statistically significant. The intercept estimate is also very large and its standard error is also very large. We have the standard error of the other coefficients are also quite large. There seems to be problems with the model.

Some of the problems can be caused by multicollinearity. Now, we look at multicollinearity diagnostics.

```{r, include = FALSE}
library(olsrr)
```

```{r}
ols_coll_diag(model)
```

We see that the variance inflation factors are all above 100, which is very high and this indicates severe multicollinearity. The overall average condition index is also high, which indicates potential multicollinearity problems. 

Now, we try fitting a ridge regression model:

```{r, include = FALSE}
library(glmnet)
```

```{r}
y <- Bodyfat$Fat
x <- data.matrix(Bodyfat[, c('Tricepts', 'Thigh', 'Midarm')])

modelR <- glmnet(x, y, alpha = 0)

cv_model <- cv.glmnet(x, y, alpha=0)

plot(cv_model)
```
This plot shows that the optimal value to choose for the constant in the penalty term is somewhere betwen 0 and 2. We now compute the actual value of the constant.

```{r}
best_lambda <- cv_model$lambda.min
best_lambda
```
We find that the best lambda value is 0.4370159. Now, we use this to fit the ridge regression model.

```{r}
best_model <- glmnet(x, y, alpha = 0, lambda = best_lambda)

coef(best_model)
```
We find that the ridge regression model is then $Fat = -0.951 - 0.438 \cdot Tricepts + 0.437 \cdot Thigh - 0.119 \cdot Midarm$.

Now, we find the $R^2$ value of the model:

```{r}
y_predicted <- predict(modelR, s = best_lambda, newx = x)

sst <- sum((y - mean(y))^2)

sse <- sum((y - y_predicted)^2)

rsq <- 1 - sse/sst

rsq
```

We obtain an $R^2$ value of 0.7789.


# Building the regression model

In this section, we talk about selecting the best model when given several predictor variables. We discuss how to select a subset of all predictors such that the model is a good fit for the data.

If $p-1$ predictors are available, then there will be $2^p - 1$ possible models which can be constructed.

## All possible regressons

This method is self explanatory. We consider all $2^p - 1$ possible models.

We talk about criteria for model selection. There are several criteria to consider when we pick a model.

### $R^2$

The $R^2$ criteria chooses the model with the largest value of explained variation. A plot of the mean square residual vs. the number of variables in the model will appear as a parabola with the last entry "curving" up a bit. This is the one with all variables in. One may draw a horizontal line parallel to the x-axis. The point where it meets the parabola determines the best fitting model since it will be as good as when all variables are included.

### Adjusted $R^2$

An adjusted $R^2$ takes into account the values of $n$ and $p$

\begin{equation*}
  \begin{split}
    R^2_{a, p} 
    &= 1 - (\frac{n-1}{n-p})\frac{SSE}{SSTO} \\
    &= 1- \frac{MSE(p)}{SSTO/(n-1)}
  \end{split}
\end{equation*}

The criteria minimum $MSE(p)$ and maximum adjusted $R^2$ are equivalent. The adjusted $R^2$ does not necessarily increase as additional explanatory variables are added into the model. This value is similar to the $R^2$ value except that it can be negative.

### Mallow's $C_p$

Now, we talk about another criteria that is used for model selection. We derive the Mallow's $C_p$ criterion first.

Suppose that the true model has $q$ predictor variables $$\boldsymbol{Y} = \boldsymbol{X_{q}\beta_{q} + \epsilon}$$

Suppose instead we fit a model using only $p$ predictor variables. Let $\boldsymbol{H_p}$ be the hat matrix using only $p$ variables. The bias for the i-th fitted value is $$E[\hat{Y_i}] - \mu_i$$ where $\mu_i$ is the true mean. Consequently, $$E[(\hat{Y_i} - \mu_i)^2] = (E[\hat{Y_i}] - \mu_i)^2 + Var[\hat{Y_i}]$$

The total mean squared error for all the fitted values divided by $\sigma^2$ is $$\Gamma_p = \frac{1}{\sigma^2}(\sum_{i}(E[\hat{Y_i}] - \mu_i)^2 + \sum_{i}Var[\hat{Y_i}])$$

We may estimate $\sigma^2$ by the $MSE$ when all the variables are included.

The vector of residuals becomes $$\boldsymbol{e_p} = \boldsymbol{(I - H_p)Y}$$ and the error sum of squares is $$SSE_p = \boldsymbol{e_p^Te_p}$$

It follows that $$bias = E[\boldsymbol{e_p}] = \boldsymbol{(I - H_p)E[Y]} = E[\boldsymbol{Y}] - E[\boldsymbol{\hat{Y}}]$$ since $E[\boldsymbol{H_pY}] = \boldsymbol{H_p}E[\boldsymbol{Y}] = E[\boldsymbol{\hat{Y}}]$. 

When $p = q$, $bias = E[\boldsymbol{Y}] - E[\boldsymbol{\hat{Y}}] = 0$

Now, using the idempotency of $\boldsymbol{(I - H_p)}$, we get

\begin{equation*}
  \begin{split}
    E[SSE_p]
    &= E[\boldsymbol{e_p^Te_p}] \\
    &= E[\boldsymbol{Y^T(I - H_p)Y}] \\
    &= E[\boldsymbol{Y^T(I-H_p)(I-H_p)Y}] \\
    &= \sigma^2Trace[\boldsymbol{I-H_p}] + (bias)^T(bias) \\
    &= \sigma^2(n - p) + \sum_{i}(E[\hat{Y_i}] - \mu_i)^2
  \end{split}
\end{equation*}

The total mean squared error for all the fitted values divided by $\sigma^2$ is then

\begin{equation*}
  \begin{split}
    \Gamma_p
    &= \frac{1}{\sigma^2}E[\sum_{i}(E[\hat{Y_i}] - \mu_i)^2 + \sum_{i}Var[\hat{Y_i}]] \\
    &= \frac{1}{\sigma^2}(E[SSE_p] - \sigma^2(n-p) + \sum_{i}Var[\hat{Y_i}]) \\
    &= \frac{1}{\sigma^2}E[SSE_p] - (n - p) + \frac{1}{\sigma^2}\sum_{i=1}^{n}Var[\hat{Y_i}] \\
    &= \frac{1}{\sigma^2}E[SSE_p] - (n - p) + \frac{1}{\sigma^2}\sigma^2 \sum_{i=1}^{n}[\boldsymbol{x_i^T(X^TX)^{-1}x_i}] \\
    &= \frac{1}{\sigma^2}E[SSE_p] - (n - p) + \frac{1}{\sigma^2}\sigma^2Trace[\boldsymbol{H_p}] \\
    &= \frac{1}{\sigma^2}E[SSE_p] - (n - p) + \frac{1}{\sigma^2}\sigma^2p \\
    &= \frac{1}{\sigma^2}E[SSE_p] - (n - p) + p
  \end{split}
\end{equation*}

Hence, we get that $$\Gamma_p = \frac{1}{\sigma^2}E[SSE_p] - (n - 2p)$$

If we estimate $\sigma^2$ by MSE with all predictors and $E[SSE_p]$ by $SSE_p$ then the Mallow's criteria becomes $$C_p = \frac{SSE_p}{MSE} - (n - 2p)$$

If the p-term model has negligible bias, then $E[SSE_p] \approx (n-p)\sigma^2$ and $C_p \approx p$. Mallows proposed a graphical technique to find the optimal subset. Plot $C_p$ vs. $p$ for all possible regressions. Models with small bias will be close to the line $C_p = p$ while those with large bias will be above the line. Values below the line are considered to have no bias.

### Akaike Information Criterion

Akaike proposed a criteria based on minimizing the expected entropy of the model, which is essentially a penalized likelihood measure. In the case of ordinary least squares regression, it becomes $$AIC_p = n\ln(SSE_p) - n\ln(n) + 2p$$ 

As more variables are included, $AIC_p$ decreases and the issue becomes whether or not the decrease justifies the inclusion of more variables.

### Schwartz's Bayesian Criterion

A Bayesian extension of the Akaike criterion was proposed by Schwartz $$BIC_{Sch} = n\ln(SSE_p) - n\ln(n) + p(\ln(n))$$

This criterion places a greater penalty than the Akaike criterion and it is the one used by R.

### Prediction Sum of Squares Criterion (PRESS)

Sometimes regression equations are used to predict future values. A different criteria used is to select the model which minimizes $$PRESS_p = \sum_{i} [Y_i - \hat{Y}_{(i)}]^2$$ where $\hat{Y}_{(i)}$ is the fitted value when the i-th observation is deleted.

## Forward Selection

In the previous section, we talked about all possible regressions and how to use different criteria to select the best model out of all. In this section and the next 2 sections, we talk about iterative models that can be helpful in selecting the best model. These methods work better with many predictors since using all possible regressions can be computationally difficult with $2^p - 1$ models when $p$ is large. 

We first talk about Forward Selection. We start with no regressors in the model (i.e. intercept only). We compute the standardized student t statistic for each variable and choose the one with the greatest absolute value to include in the model. This is also the variable that has the largest simple correlation with the response. We also choose a pre-selected critical $F$ value, denote it by $F_{in}$.

With the variable selected, we choose the next variable using the same criteria as the previous step after adjusting for the effect of the first variable selected. The criteria makes use of partial correlations which are computed between the residuals from the previous step and the residuals from the regressions of the other regressors on $X_j$, that is residuals from $\hat{Y} = \hat{\beta_0} + \hat{\beta_1}X_1$ and residuals from $\hat{X_j} = \hat{\alpha_{0j}} + \hat{\alpha_{1j}}X_1$ for $j = 2, ..., k$. In short, we plot the residuals from the 2 models and we choose $X_j$ such that it maximizes the partial correlation.

If $X_2$ is selected, it implies that the largest partial $F$ statistic is $$F = \frac{SSR(X_2|X_1)}{MSE(X_1, X_2)}$$
If $F > F_{in}$, then $X_2$ is entered into the model. Then we keep going until either all variables are included or when the largest partial $F$ statistic no longer exceeds $F_{in}$.

One drawback to this method is that once a variable is entered, it doesn't get eliminated. This means that the variable can turn out to not be significant in the chosen model but it doesn't get removed.

## Backward Elimination

This model is the opposite of Forward Selection discussed in the section above. We start with all regressors in the model (i.e. the full model with all predictors included). Compute the partial $F$ statistic for each regressor as if it were the last one to enter the model. We compare the smallest partial $F$ with a pre-selected $F_{out}$ value. If it is smaller, then that variable is removed from the model. The procedure is repeated until the smallest partial $F$ statistic is not less than $F_{out}$ or when all variables are removed. Backward elimination is often preferred to forward selection because it begins with all the variables in the model.

One drawback is that once a variable is eliminated, it cannot be added back into the model. This can be a problem if that variable is significant in the reduced model chosen.

## Stepwise Selection

Stepwise Selection combines the 2 approaches Forward Selection and Backward Elimination. It is a modification of forward selection in that it reassesses each of the regressors already in the model to see if it has become redundant. We need to pre-select $F_{in}$ and $F_{out}$. Usually, we choose $F_{in} > F_{out}$ so that it becomes more difficult to add a variable than to remove it.

## Example: Hald Cement Data Analysis

We now look at an example that examines the Hald Cement data. We first load the data:

```{r}
Cement <- read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Hald Cement Data.txt", header=TRUE, sep="\t")
```

```{r, include=FALSE}
library(olsrr)
```

```{r}
names(Cement)
plot(Cement)
cor(Cement)
```
From looking at the correlation matrix and the plot matrix, we find that $Y$ is strongly correlated with $X_1$, $X_2$, and $X_4$. There also appears to be multicollinearity between $X_1$ and $X_3$ and between $X_2$ and $X_4$.

Now, we look at multicollinearity diagnostics:

```{r}
model <- lm(Y~., data=Cement)
ols_coll_diag(model)
```

From looking at the VIFs, we have that there is strong multicollinearity with $X_2$ and $X_4$ when all predictors are fitted. This means that we should pick a model that doesn't necessarily include all variables.

We first try running all possible regressions:

```{r}
k = ols_step_all_possible(model)
k
```
From looking at the $R^2$, adjusted $R^2$, and the Mallow's $C_p$ criterion, we find that the model involving $X_1$, $X_2$, and $X_4$ is the best model. However, we have seen that there is strong multicollinearity when both $X_2$ and $X_4$ are included in the model. So we can choose a further reduced model. The model with $X_1$, $X_2$, and $X_3$ also looks like a good model since it has a high $R^2$ value and the $C_p$ value is close to the number of predictors. However, from looking at the correlation matrix, there's high correlation between $X_1$ and $X_3$. We see that the model using only $X_1$ and $X_2$ has a high $R^2$ and adjusted $R^2$ and the $C_p$ criterion is also close to the number of predictors. Therefore, the model involving only $X_1$ and $X_2$ is a good model to choose.

```{r}
plot(k)
```

Now, we run best subset selection and choose the best model using it.

```{r}
k = ols_step_best_subset(model)
k
```

We find that the best subset is the subset $X_1$, $X_2$, and $X_4$. However, as mentioned previously, there may be multicollinearity issues with the model. The next best model is the model with 2 predictors $X_1$ and $X_2$, which has less multicollinearity issues. This means that it's better to choose the subset with $X_1$ and $X_2$.

Now, we run forward selection

```{r}
ols_step_forward_p(model, details=TRUE)
```

We have seen that the forward selection method selected the variables $X_1$, $X_2$, and $X_4$ in the model. It selected $X_4$ first, then $X_1$, finally $X_2$. This is the same as the best model selected using all possible regressions. Again, there is multicollinearity issues if this model is selected.

Now, we run backward elimination:

```{r}
ols_step_backward_p(model, details = TRUE)
```

We can see that backward elimination has eliminated the variable $X_3$ from the model, so we have that the method selected $X_1$, $X_2$, and $X_4$ in the model.

Finally, we run stepwise selection:

```{r}
ols_step_both_p(model, details = TRUE)
```

We see that stepwise selection has added the variables $X_1$, $X_2$, and $X_4$. It didn't eliminate any variables. Therefore, it also selected $X_1$, $X_2$, and $X_4$ to be used in the model.

# Logistic Regression

In all the sections above, we talked about ordinary linear regression, where we assume that the model is linear in the parameters. In this section, we consider the case where the response variable $Y_i$ is binary and the appropriate model and inferences.

## Logistic Regression Model

Sometimes the response variable is discrete. For example, we may wish to model gender or to estimate the likelihood that a person is wearing a life jacket. 

We first consider the linear regression model $$Y_i = \beta_0 + \beta_1X_i + \epsilon_i$$ where 

$$
  Y_i = \left \{
  \begin{array}{ll}
    1  \quad \text{with probability } \pi_i \\
    0  \quad \text{with probability } 1 - \pi_i \,
  \end{array}
  \right.
$$

Then we have $E[Y_i] = \pi_i$ since $Y_i \sim Ber(\pi_i)$.

In this case, the usual least squares fitting approach is problematic because the assumptions are no longer reasonable:

\begin{itemize}
  \item the variance of $Y_i$, where $Var[Y_i] = \pi_i(1 - \pi_i)$ is not constant since it changes based on $\pi_i$
  \item the error terms are not normally distributed (since $Y_i$ follow a Bernoulli distribution)
  \item there is no guarantee that the fitted model will force the estimate $\hat{Y_i}$ to be in the interval $(0, 1)$
\end{itemize}

The logistic distribution has density $$f(x) = \frac{e^x}{(1+e^x)^2} \text{, } -\infty < x < \infty$$ and cumulative distribution function $$F(t) = \frac{e^t}{1 + e^t}$$

We can show $E[X] = 0$ and $Var[X] = \frac{\pi}{3}$.

Suppose that a random variable $Y$ is binary with 

$$
  Y_i = \left \{
  \begin{array}{ll}
    1  \quad \text{if } \beta_0^* + \beta_1^*X_i + \epsilon_i^* < k\\
    0  \quad \text{if } \beta_0^* + \beta_1^*X_i + \epsilon_i^* > k \,
  \end{array}
  \right.
$$

for some constant $k$ where $\epsilon_i^*$ has a logistic distribution.

Then we get 

\begin{equation*}
  \begin{split}
    \pi_i
    &= P(Y_i = 1) \\
    &= P(\beta_0^* + \beta_1^*X_i + \epsilon_i^* < k) \\
    &= F(k - \beta_0^* - \beta_1^*X_i) \\
    &= F(\beta_0 + \beta_1X_i) \\
    &= \frac{exp(\beta_0 + \beta_1X_i)}{(1 + exp(\beta_0 + \beta_1X_i))}
  \end{split}
\end{equation*}

where $\beta_0 = k - \beta_0^*$ and $\beta_1 = -\beta_1^*$

So we get that $$E[Y_i] = \pi_i = \frac{exp(\beta_0 + \beta_1X_i)}{(1 + exp(\beta_0 + \beta_1X_i))}$$. This is the structure of the logistic regression model.

It is common practice to model the logarithm of the odds ratio $$\log(\frac{\pi_i}{1-\pi_i}) = \log(\frac{P(Y_i = 1)}{1 - P(Y_i = 1)}) = \beta_0 + \beta_1X_i$$

The estimation of the parameters is based on maximizing the likelihood:

\begin{equation*}
  \begin{split}
    \prod_{i}f(y_i) 
    &= \prod_{i}\pi_i^{y_i}(1 - \pi_i)^{1 - y_i} \\
    &= \prod_{i} [(\frac{\pi_i}{1 - \pi_i})^{y_i}(1 - \pi_i)] \\
  \end{split}
\end{equation*}

Then we take the log likelihood.

$$logL = \sum y_i\log(\frac{\pi_i}{1 - \pi_i}) + \sum \log(1 - \pi_i) = \sum y_i(\beta_0 + \beta_1X_i) - \sum \log(1 + exp(\beta_0 + \beta_1X_i))$$

There is no closed form solution. Instead, numerical methods are used to obtain a solution for $\beta_0$ and $\beta_1$, denoted by $b_0$ and $b_1$ and we get $$\hat{\pi_i} = \frac{exp(b_0 + b_1X_i)}{(1 + exp(b_0 + b_1X_i))}$$

To interpret the parameters in the logistic regression model, let us consider the fitted value at a specific value of $X$, say $X_0$. Then the difference between the log odds at $X_0 + 1$ and the log odds at $X_0$ is $$logodds(X_0 + 1) - logodds(X_0) = \hat{\beta_1}$$ meaning that for every unit increase in $X_0$, the log odds $\log(\frac{\pi_i}{1 - \pi_i})$ at $X_0$ increases by an average of $\hat{\beta_1}$ units. 

Taking the antilogarithms, we obtain the odds ratio $$\hat{O}_R = e^{\hat{\beta_1}}$$

The odds ratio is the estimated increase in the probability of successes associated with a one unit change in the value of the predictor variable. For a change of $d$ units, the odds ratio becomes $$\hat{O}_R = e^{d\hat{\beta_1}}$$

## Repeat Observations

In the section above, we looked at when we took individual observations at each $X$ value. Now, we look at repeat observations. Suppose that we have repeat observations at each of the leels of the $X$ variables and set $Y_i$ to be the number of 1s observed for the i-th observation. Let $n_i$ be the number of trials at each observation. Then $Y_i \sim Bin(n_i, \pi_i)$. In that case, estimation is done by maximizing

\begin{equation*}
  \begin{split}
    \log L(\beta_0, \beta_1)
    &= \log \prod_{i=1}^{n} {n_i \choose y_i}\pi_i^{y_i}(1-\pi_i)^{n_i - y_i} \\
    &= \sum_{i=1}^{n}(\log({n \choose y_i}) + y_i(\log(\pi_i)) + (n_i - y_i)\log(1 - \pi_i))
  \end{split}
\end{equation*}

## Multiple Logistic Models

We can also involve multiple predictors in the logistic regression model. Specifically, we have $$\boldsymbol{X^T\beta} = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \dots + \beta_{p-1}X_{i, p-1}$$ and $$E[Y] = \frac{exp(\boldsymbol{X^T\beta})}{1 + exp(\boldsymbol{X^T\beta})}$$ so that $$\log(\frac{\boldsymbol{\pi}}{1 - \boldsymbol{\pi}}) = \boldsymbol{X^T\beta}$$

## Inference on model parameters

Now, we talk about inference on the parameters of the model. There are several diagnostics that we can use to make inferences. 
The maximum likelihood estimators are approximately normally distributed with variances and covariances that are functions of the second order partial derivatives of the likelihood function.

Let $\boldsymbol{G} = (\frac{\partial^2 L(\beta)}{\partial \beta_i \partial \beta_j}) \equiv (g_{ij})$ lebeled the Hessian where $$\log L(\beta) = \sum_{i=1}^{n}Y_i(X_i^T\beta) - \sum_{i=1}^{n} \log (1 + e^{X_i^T\beta})$$

The linear predictor is $\boldsymbol{X^T\hat{\beta}}$ and the fitted value is $\hat{Y_i} = \hat{\pi_i} = \frac{exp(X_i^T\hat{\beta})}{(1 + exp(X_i^T\hat{\beta}))}$.

It can be shown that $$E[\boldsymbol{b}] = \boldsymbol{\beta}$$

The variance estimate is given by $$Var[\boldsymbol{b}] = (\boldsymbol{X^TVX})^{-1}$$ where $\boldsymbol{V}$ is a diagonal matrix with $V_{ii} = n_i\hat{\pi_i}(1 - \hat{\pi_i})$. Moreover, we have that $$\frac{b_k - \beta_k}{s[b_k]} \sim N(0, 1)$$ for $k = 0, ..., p-1$, which is used for testing and constructing confidence intervals. The test statistic to test $H_0: \beta_k = 0$ is $$\frac{b_k}{s[b_k]} \sim N(0, 1)$$ and a confidence interval is $$b_k \pm z_{\alpha/2} \cdot s[b_k]$$

To test whether several coefficients are 0, we make use of the likelihood ratio test whereby we compare the full mode l (FM) with the reduced model (RM). Let $$LR = -2\log[\frac{L(RM)}{L(FM)}]$$ If the reduced model is correct, $LR$ follows asymptotically a chi-squared distribution with degrees of freedom equal to the difference in the number of parameters between the full and reduced models. $df_{RM} - df_{FM} = (n-q) - (n-p)$. We reject for large values (i.e. $LR > \chi^2_{p-q}$).

In the present situation, for the simple logistic model, the Full model is the one that has been fitted whereas the Reduced model is the one with constant probability of success $$E[Y] = \pi = \frac{e^{\beta_0}}{1 + e^{\beta_0}}$$

Under RM, the maximum likelihood estimate of the constant probability of success is $Y/n$, where $Y$ is the number of successes that has been observed and $n$ is the total number of observations. Hence, we have $$\log L(RM) = [y\log(y) + (n-y)\log(n-y) - n\log(n)]$$

Hence, the likelihood ratio statistic for testing significance of regression is $$LR = 2(\sum_{i=1}^{n} y_i\log(\hat{\pi}) + \sum_{i=1}^{n}(n_i - y_i)\log(1 - \hat{\pi_i})) - 2(y\log(y) + (n-y)\log(n-y) -n\log(n)$$ We reject the null hypothesis that the regression is non-significant if $LR$ is large.

## Test for Goodness of Fit

We want to first make sure that a logistic regression model is a good fit for the data before we accept it. This is analogous to the usual lack of fit testing regression context. In that context, we required repeat observations as we do here. We would like to test $$H_0: E[\boldsymbol{Y}] = (1 + e^{-\boldsymbol{X^T\beta}})^{-1}$$ $$H_1: E[\boldsymbol{Y}] \neq (1 + e^{-\boldsymbol{X^T\beta}})^{-1}$$

Here, we will make use of a Pearson Chi-square goodness of fit test. The expected number of successes is $n_i\hat{\pi_i}$ and the expected number of failures is $n_i(1 - \hat{\pi_i})$. The Pearson Chi-square test statistic is $$\chi^2 = \sum_{i=1}^{n}(\frac{(Y_i - n_i\hat{\pi_i})^2}{n_i\hat{\pi_i}} + \frac{((n_i - Y_i) - n_i(1 - \hat{\pi_i}))^2}{n_i(1 - \hat{\pi_i})})$$

We reject the null hypothesis whenever the test statistic $\chi^2 > \chi^2_{\alpha, n-p}$, where $n$ is the number of groups and $p$ is the number of predictors used.

When there are no replicates on the regressor variables, observations can be grouped to perform a goodness-of-fit test called the Hosmer-Lemeshow test. We group the observation into $g$ groups. Let $O_j$ and $N - O_j$ be the observed number of successes and failures respectively in group $j$, where $N_j$ is the total number of observations in the group. The estimated probability of success $\hat{\pi_j}$ in the j-th group is the average estimated success probability. Then the Hosmer-Lemenshow test statistic is $$HL = \sum_{j=1}^{g} \frac{(O_j - N_j\hat{\pi_j})^2}{N_j\hat{\pi_j}}+\sum_{j=1}^{g}\frac{(N_j - O_j - N_j(1-\pi_j))^2}{N_j(1 - \hat{\pi_j)}} = \sum_{j=1}^{g}\frac{(O_j - N_j\hat{\pi_j})^2}{N_j\hat{\pi_j}(1 - \hat{\pi_j})}$$ The HL statistic follows a chi-squared distribution with $g-1$ degrees of freedom. We reject the null hypothesis for large values of the HL statistic.

Now, we talk about the Deviance Goodness of Fit Test. This test is based on the likelihood ratio test whereby we consider the reduced and full models. Here we compare the current model to a saturated model whereby each observation (or group when $n_i > 1$) has its own probability of success estimated by $Y_i/n_i$.

Under the reduced model, we have $$E[Y_i] = (1 + e^{-\boldsymbol{X^T\beta}})^{-1}$$ whereas under the full model (also called the saturated model) we have $$E[Y_i] = \pi_i$$

The deviance goodness of fit statistic, also called Deviance, is given by 

\begin{equation*}
  \begin{split}
    DEV(X_0, X_1, \dots, X_{p-1}) 
    &= -2[\log L(RM) - \log L(FM)] \\
    &= -2\sum_{i=1}^{n} [Y_i \log(\frac{Y_i}{n_i\hat{\pi_i}})] - 2\sum_{i=1}^{n}[(n_i - Y_i) \log(\frac{n_i - Y_i}{n_i(1 - \hat{\pi_i})})]
  \end{split}
\end{equation*}

We reject the null hypothesis that $E[Y] = (1 + e^{-\boldsymbol{X^T\beta}})^{-1}$ for $DEV > \chi^2_{n-p}$. The deviance in logistic regression plays an analogous role to the residual mean squares in ordinary regression.

## Diagnostic Measures for Logistic Regression

Now, we talk about diagnostic measures for logistic regression. In section 7, we talked about several diagnostic measures for the ordinary linear regression model. However, the diagnostic measures are different for the logistic regression model. 

We shall consider the ungrouped case only. Residuals in that case can be used to diagnose the adequacy of the fitted model. The ordinary residuals are defined as $$e_i = Y_i - n_i\hat{\pi_i}$$

These do not have constant variance. The deviance residual is for $i = 1, ..., n$, $$d_i = \pm(2[Y_i\log(\frac{Y_i}{n_i\hat{\pi_i}}) + (n_i - Y_i)\log(\frac{n_i - Y_i}{n_i(1 - \hat{\pi_i})})])^{1/2}$$ where the sign of $d_i$ is the same as the sign of $e_i$.

Similarly, we may compute the standardized Pearson residuals $$r_{P_i} = \frac{Y_i - n_i\hat{\pi_i}}{\sqrt{n_i(\hat{\pi_i}(1 - \hat{\pi_i}))}}$$ which do not have unit variance

We can also compute the studentized Pearson residuals $$sr_{P_i} = \frac{r_{P_i}}{\sqrt{1 - h_{ii}}}$$ where $h_{ii}$ is the i-th diagonal element of the hat matrix $$\boldsymbol{H = V^{1/2}X(X^TVX)^{-1}X^TV^{1/2}}$$ and $\boldsymbol{V}$ is the diagonal matrix with $V_{ii} = n_i\hat{\pi_i}(1 - \hat{\pi_i})$

A plot of the deviance and the studentized Pearson residuals are useful to check for outliers. A normal probability plot of the deviance residuals can be used to check for the fit of the model and for outliers. A plot of the deviance vs. the estimated probability of success can be used to determine where the model is poorly fitted, at high or low probabilities.

For a good model, $E[Y_i] = \pi_i$ and plots of $sr_{P_i}$ vs. $\hat{\pi_i}$ and $sr_{P_i}$ vs. linear predictor $\boldsymbol{X_i^T\hat{\beta}}$ should show a smooth horizontal Lowess line through 0. Similarly for a plot of $d_i$ vs. $\hat{\pi_i}$ and $d_i$ vs. linear predictor $\boldsymbol{X_i^T\hat{\beta}}$.

In order to flag influential cases, we consider deleting one observation at a time and measuring its effect on both the $\chi^2$ ajd $DEV$ statistics. Plots of these vs. $i$ will show spikes for influential observations. Similarly for plots vs. $\hat{\pi_i}$.

Cook's distance measures the standardized change in the linear predictor $\boldsymbol{X^T\beta}$ when the i-th case is deleted. Indexed plots of Cook's distance identify cases that have a large influence on the fitted predictor. Indexed plots of the leverage values $h_{ii}$ help to identify ourliers in the $X$ space. In all cases, visual assessments are needed because there is no actual rule of thumb for flagging outlier cases.

## Example: Programming Experience Data

We now look at an example that looks at the Programming Experience Dataset. 

We first load the data:

```{r}
program <- read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Program experience.txt", header=TRUE, sep = '\t')
program
```
```{r}
mlogit <- glm(Success~Experience, data=program, family = "binomial")
summary(mlogit)
```
From the summary data above, we find that if we model the success probability against the number of years of programming experience, we find that a logistic regression model is $$\hat{\pi_i} = \frac{exp(-3.05970+0.16149X_i)}{1 + exp(-3.05970 + 0.16149X_i)}$$ where $X_i$ is the number of years of programming experience. We find that if we test $H_0: \beta_1 = 0$ against $H_1: \beta_1 \neq 0$, then we can reject $H_0$ at the 0.05 level. This means that there is a significant relationship between programming experience and success probability.

Now we construct confidence intervals of the model parameters:

```{r}
confint(mlogit)
```
A 95\% confidence interval for $\beta_0$ is $[-6.037, -0.916]$ and a 95\% confidence interval for $\beta_1$ is $[0.050, 0.314]$.

Now, we construct a confidence interval of the odds ratio:

```{r}
exp(cbind(OR = coef(mlogit), confint(mlogit)))
```
Therefore, a 95\% confidence interval for the odds ratio is $[1.052, 1.369]$.


# Poisson Regression

In the section above, we talked about Logistic regression, which is used to model binary response variables. Now, we talk about Poisson regression, which is used to model counting data $Y$ such that $Y_i \sim Pois(\mu_i)$ with mean $\mu_i$ and variance $\mu_i$.

The probability mass function of the Poisson distribution is $$f(y) = \frac{\mu^y}{y!}e^{-\mu}$$ for $y = 0, 1, ...$.

The model is then given as $$Y_i = \mu_i + \epsilon_i$$ for $i = 1, ..., n$.

We have $E[Y_i] = \mu_i$ and $Var[Y_i] = \mu_i$ so the variance is not constant.

We assume that there is a link function $g(\mu_i)$ that specifies the mean which may be one of the following:

 $$g(\mu_i) = \mu_i = X_i^T\beta$$
 $$g(\mu_i) = \ln(\mu_i) = X_i^T\beta$$
 
The first one is the identity link and the second one is the log link.

The log likelihood is given by $$\log L(y, \beta) = \sum_{i = 1}^{n} y_i \log(\mu_i) - \sum_{i=1}^{n} \mu_i - \sum_{i=1}^{n} \log(y!)$$

This equation is used for the estimation of parameters. There is no closed form solutions. Numerical methods are required to find the solution.

The fitted Poisson model is then 

\begin{equation*}
  \begin{split}
    \hat{Y_i}
    &= g^{-1}(X_i^T\beta) \\
    &= \left \{
  \begin{array}{ll}
    X_i^T\hat{\beta}  \quad \text{identity link } \\
    exp(X_i^T\hat{\beta})  \quad \text{log link } \,
  \end{array}
  \right.
  \end{split}
\end{equation*}

Inference on the Poisson model is conducted as in the case of the logistic model.

## Example: Aircraft Damage Data

We first load the data:

```{r}
Aircraft <- read.table("/System/Volumes/Data/MAT 3375/Summary Sheet/Aircraft Damage Data.txt", header=TRUE, sep="\t")
summary(Aircraft)
```

```{r}
mpoisson = glm(Y~X1+X2+X3, data=Aircraft, family = poisson(link="log"))
summary(mpoisson)
```
We find that the fitted Poisson regression model is $Y_i = exp(-0.406 + 0.569X_{i1} + 0.165X_{i2} - 0.013X_{i3})$. We also find that there is a significant relationship between $Y$ and $X_2$, but not between $Y$ and $X_1$ and between $Y$ and $X_3$. We find that the Residual deviance is 25.953, which would result in a p-value between 0.1 and 0.5, which means that the Poisson model is a good fit for the data.

# Generalized Linear Models

Both the Logistic and Poisson models are examples of the generalized linear model (GLM). The generalized linear model involces a linear predictor, a link function, and a distribution for the response variable. The response variable is assumed to have a distribution which is a member of the exponential family of distributions.

$$f(y_i, \theta_i, \phi) = exp(\frac{y_i\theta_i - b(\theta_i)}{a(\phi)} + h(y_i, \phi))$$

Here, we get that $\mu_i = E[Y_i] = \frac{db(\theta_i)}{d\theta_i}$ and $Var[Y_i] = \frac{d^2 b(\theta_i)}{d\theta_i^2} a(\phi)$

The basic idea is to develop a linear model for a function of the mean. Set $\eta_i = g(\mu_i) = \boldsymbol{X_i^T\beta}$. The function $g$ is called the link function. 

For the logistic regression model, we have that the density of the response variable is 

$$f(y_i, \theta_i, \phi) = {n_i \choose \pi_i}\pi_i^{y_i}(1 - \pi_i)^{n_i - y_i} = exp((y_i\log(\frac{\pi_i}{1 - \pi_i}) + n_i\log(1 - \pi_i)) + \log{n_i \choose y_i})$$ where $\theta_i = \log(\frac{\pi_i}{1 - \pi_i})$, $b(\theta_i) = -(n_i\log(1 - \pi_i)) = -n_i\log(1 - \pi_i) = n_i\log(1 + \frac{\pi_i}{1-\pi_i}) = n_i\log(1 + e^{\theta_i})$, $\phi = 1$, $a(\phi) = 1$, and $h(y_i, \theta_i) = \log{n_i \choose y_i}$

The link function is $g(\mu_i) = \log(\frac{\mu_i}{1 - \mu_i})$.

For the Poisson regression model, we have that the density of the response variable is

$$f(y_i, \theta_i, \phi) = \frac{e^{-\lambda_i}\lambda^{y_i}}{y_i!} = exp((y_i\log(\lambda_i) - \lambda_i) - \log(y_i!))$$ where $\theta_i = \log(\lambda_i)$, $b(\theta_i) = \lambda_i = e^{\theta_i}$, $\phi = 1$, $a(\phi) = 1$, and $h(y_i, \theta_i) = \log(y_i!)$

The link function is either the identity link or the log link.